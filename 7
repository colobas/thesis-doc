---
title: Variational Mixture of Normalizing Flows
author: Guilherme Grij√≥ Pen Freitas Pires
logo: '`\includegraphics[width=0.2\textwidth]{figures/logo_ist.jpg}`{=latex}'
toc: true
---
# Introduction and Motivation

## Introduction and Motivation

\begin{itemize}
\onslide<1->{\item Deep generative models: an active area of research}
    \begin{itemize}
        \onslide<2->{\item \textbf{Implicit distributions}: Generative adversarial networks
            {\scriptsize \color{black!50} \autocites{GAN}}, Variational Autoencoder {\scriptsize \color{black!50} \autocites{vaepaper}}
            \begin{itemize} \item No explicit access to the density function \end{itemize}}
        \onslide<3->{\item \textbf{Explicit distributions}: Normalizing flows {\scriptsize \color{black!50} \autocites{shakir_nf}}
            \begin{itemize}
                \item Explicit access to the density function
                \item No approach to introduce discrete structure (multi-modality)
            \end{itemize}}
    \end{itemize}
\end{itemize}

## Introduction and Motivation: Goal

\begin{itemize}
\onslide<1->{\item Goal of this work: mixture of flexible distributions.}
\onslide<2->{\item Two questions:}
    \begin{itemize}
        \onslide<3->{\item What should the mixture components be?}
        \onslide<4->{\item How should their parameters be estimated?}
    \end{itemize}
\end{itemize}

## Outline

\begin{itemize}
\onslide<1->{\item Mixture Models}
\onslide<2->{\item Variational Inference}
    \onslide<3->{\begin{itemize} \item The chosen framework for estimating the parameters of the proposed model\end{itemize}}
\onslide<4->{\item Normalizing Flows}
    \onslide<5->{\begin{itemize} \item The chosen family for the mixture model components \end{itemize}}
\onslide<6->{\item Variational Mixture of Normalizing Flows}
\onslide<7->{\item Experiments and results}
\onslide<8->{\item Conclusions and future work}
\end{itemize}

# Mixture Models

## Mixture Models: Definition

\begin{itemize}
    \onslide<1->{\item Mixture model: used to model data that contains subgroups.}
    \onslide<2->{\item \q{Subgroup-conditional} distributions (typically) in the same family}
\end{itemize}
\onslide<3->{\centering \includegraphics[width=0.6\textwidth]{figures/selector-mixture.png}}


## Mixture Models: Plate diagram

\centering
\includegraphics[width=0.7\textwidth]{figures/plate_diagram2.png}

## Mixture Models: Mixture of Gaussians

\centering
\onslide<1->{\includegraphics[width=0.475\textwidth]{figures/mog.png}}
\hfill
\onslide<2->{\includegraphics[width=0.475\textwidth]{figures/mog_observed.png}}

# Normalizing Flows

## Normalizing Flows: Change of Variables

\onslide<1->{Given
\begin{align*}
\begin{cases}
    \bm{z} \sim p(\bm{z}) \\
    \bm{x} = g(\bm{z}; \bm\theta)
\end{cases}
\end{align*}}
\onslide<2->{then:
\begin{align*}
    f_X(\bm{x}) &= f_Z(g^{-1}(\bm{x};\bm\theta))\Big|\det\Big(\frac{d}{d\bm{x}}g^{-1}(\bm{x};\bm\theta)\Big)\Big| \\
    &= f_Z(g^{-1}(\bm{x};\bm\theta))\Big|\det\Big(\frac{d}{d\bm{z}}g(\bm{z};\bm\theta) \bigg{|}_{\bm{z} = g^{-1}(\bm{x};\bm\theta)}\Big)\Big|^{-1}
\end{align*}}

\onslide<3->{This can be optimized w.r.t. $\bm\theta$, to approximate an arbitrary distribution}

## Normalizing Flows: Change of Variables

\onslide<1->{Requirements for feasibility}
\begin{enumerate}
\renewcommand{\theenumi}{\Alph{enumi}}
    \onslide<2->{\item Base density: closed form and easy to sample from}
    \onslide<3->{\item Determinant of the Jacobian of $g$: computationally cheap}
    \onslide<4->{\item Gradient of \emph{B} w.r.t $\bm\theta$: computationally cheap}
\end{enumerate}

## Normalizing Flows: Change of Variables

\onslide<1->{Normalizing Flows: composition of several \q{good} transformations}
\onslide<2->{I.e., $g = h_{L-1} \odot h_{L-2} \odot ... \odot h_1 \odot h_0$}
\onslide<3->{Applying the formula to $g$, and taking the logarithm:
\begin{align*}
    \log f_X(\bm{x}) = \log f_Z(g^{-1}(\bm{x})) - \sum_{\ell=0}^{L-1} \log \Big|\det\Big(\frac{d}{d\bm{x_{\ell}}}h_{\ell}(\bm{x_\ell})\Big) \Big|. \label{eq:nflowsfinal}
\end{align*}}

## Normalizing Flows: Affine Coupling Layer

An example: Affine Coupling Layer
{\scriptsize \autocites{real-nvp}}.

\onslide<1->{Splitting $\bm{z}$ into $(\bm{z_1}, \bm{z_2})$,}
\onslide<2->{\begin{align*}
    \begin{cases}
    \bm{x_1} &= \bm{z_1} \odot \exp\big(s(\bm{z_2})\big) + t(\bm{z_2}) \\
    \bm{x_2} &= \bm{z_2}.
    \end{cases}
\end{align*}}
\onslide<3->{The respective Jacobian matrix:
\begin{align*}
    J_{f(z)} =
        \begin{tikzpicture}[decoration=brace, baseline=-\the\dimexpr\fontdimen22\textfont2\relax ]
            \matrix (m) [matrix of math nodes,left delimiter=[,right delimiter={]}, ampersand replacement=\&] {
                \mbox{\Large$\frac{\partial \bm{x_1}}{\partial \bm{z_1}}$} \& \mbox{\Large$\frac{\partial \bm{x_1}}{\partial \bm{z_2}}$} \\
                \mbox{\Large$\frac{\partial \bm{x_2}}{\partial \bm{z_1}}$} \& \mbox{\Large$\frac{\partial \bm{x_2}}{\partial \bm{z_2}}$} \\
            };
        \end{tikzpicture}
    =
        \begin{tikzpicture}[decoration=brace, baseline=-\the\dimexpr\fontdimen22\textfont2\relax ]
            \matrix (m) [matrix of math nodes,left delimiter=[,right delimiter={]}, ampersand replacement=\&] {
                \mbox{diag}\Big(\exp\big(s(\bm{z_2})\big)\Big) \& \mbox{\Large$\frac{\partial \bm{x_1}}{\partial \bm{z_2}}$} \\
                \mbox{\Large$\bm{0}$} \& \mbox{\Large$I$} \\
            };
        \end{tikzpicture}
\end{align*}
}

# Variational Inference

## Variational Inference: Preamble

\onslide<1->{Joint probability distribution $p(\bm{x}, \bm{c})$.}
\onslide<2->{$\bm{x}$ is observed and $\bm{c}$ is latent.}
\onslide<3->{%
Inference about $\bm{z}$, given $\bm{x}$, by Bayes' Law:
\begin{align*}
    p(\bm{z}|\bm{x}) &= \frac{p(\bm{x}|\bm{z})p(\bm{z})}{p(\bm{x})} \\
                     &= \frac{p(\bm{x}|\bm{z})p(\bm{z})}{\int p(\bm{x}|\bm{z}')p(\bm{z'}) d\bm{z'}}
\end{align*}
}
\onslide<4->{\textbf{Problem}: The integral is normally intractable}
\onslide<5->{\begin{itemize} \item Variational inference: an approximate
inference framework to overcome this intractability. \end{itemize}}

## Variational Inference: Goal

Given a family $q(\bm{z} ; \bm\lambda)$, find the parameters $\bm\lambda$
that minimize the Kullback-Leibler divergence between $q(\bm{z} ; \bm\lambda)$ and
$p(\bm{z}|\bm{x})$

\begin{align*}
    \bm{\lambda^{*}} = \argmin_{\bm\lambda} KL(q(\bm{z}; \bm\lambda) || p(\bm{z} | \bm{x}))
\end{align*}

## Variational Inference: ELBO

\begin{align*}
\onslide<1->{KL(q(\bm{z}) || p(\bm{z}|\bm{x})) &= \int q(\bm{z}) \log\frac{q(\bm{z})}{p(\bm{z}|\bm{x})} d\bm{z} \\}
\onslide<2->{&= \int q(\bm{z}) (\log q(\bm{z}) - \log p(\bm{z}|\bm{x})) d\bm{z} \\}
\onslide<3->{&= \int q(\bm{z}) (\log q(\bm{z}) - (\log p(\bm{x}, \bm{z}) - \log p(\bm{x}))) d\bm{z} \\}
\onslide<4->{&= \mathbb{E}_q [\log q(\bm{z})] - \mathbb{E}_q [\log p(\bm{x}, \bm{z})] + \log p(\bm{x})}
\end{align*}

\onslide<5->
Which yields the lower bound (ELBO):
\begin{align*}
    \mbox{ELBO}(q) &= \mathbb{E}_q [\log p(\bm{x}, \bm{z})] - \mathbb{E}_q [\log q(\bm{z})] \\
            &= \mathbb{E}_q [\log p(\bm{x}|\bm{z})] + \mathbb{E}_q [\log p(\bm{z})] - \mathbb{E}_q [\log q(\bm{z})]
\end{align*}

# Variational Mixture of Normalizing Flows

## VMoNF: Introduction
\onslide<1->{Is it possible to combine the ideas from the previous sections,
to obtain a mixture of flexible models?}

## VMoNF: Definition

Recall the ELBO:
\begin{align*}
    ELBO(q) &= \mathbb{E}_q [\log p(\bm{x}|\bm{z})] + \mathbb{E}_q [\log p(\bm{z})] - \mathbb{E}_q [\log q(\bm{z})]
\end{align*}

. . .

$q(z|x)$ is parameterized by a neural network.

. . .

Optimize the ELBO, by \textbf{jointly} learning the variational posterior and
the generative components.

## VMoNF: Overview

\centering
\includegraphics[width=0.7\textwidth]{figures/train_overview.png}

## VMoNF: Experiments - Pinwheel (5 wings)

\centering
\onslide<1->{\includegraphics[width=0.475\textwidth]{figures/original_pinwheel.png}}
\hfill
\onslide<2->{\includegraphics[width=0.475\textwidth]{figures/trained_pinwheel.png}}

## VMoNF: Experiments - Pinwheel (3 wings)

Trainining Animation

## VMoNF: Experiments - 2 Circles

\centering
\onslide<1->{\includegraphics[width=0.475\textwidth]{figures/original_2_circles.png}}
\hfill
\onslide<2->{\includegraphics[width=0.475\textwidth]{figures/trained_2_circles_2.png}}

## VMoNF: Experiments - 2 Circles (semi supervised)

\centering
\onslide<1->{\includegraphics[width=0.475\textwidth]{figures/labeled_2_circles.png}}
\hfill
\onslide<2->{\includegraphics[width=0.475\textwidth]{figures/trained_2_circles_semisup.png}
{\scriptsize Note: 32 labeled points, 1024 unlabeled points}}

## VMoNF: Experiments - MNIST

\centering
\includegraphics[width=0.7\textwidth]{figures/trained_mnist.png}

# Conclusions

## Conclusions and Future Work

\begin{itemize}
    \onslide<1->{\item Similar work is being pursued and published in prominent venues: \autocites{RAD}{semisuplearning_nflows}}
    \onslide<2->{\item Formally describe the reasons why the model fails in cases like the 2 circles $\to$ Topology}
        \onslide<3->{\begin{itemize} \item Investigate the effect of a consistency loss regularization term \end{itemize}}
    \onslide<4->{\item Weight-sharing between components}
    \onslide<5->{\item Balance between complexities}
    \onslide<6->{\item (Controlled) component anihilation}
\end{itemize}

##

Thank you!
