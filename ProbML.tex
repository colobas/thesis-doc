\chapter{Probabilistic Modelling}
\label{chapter:probmodel}

\section{Introduction}
\label{section:probmodelintro}
Probabilistic modelling is a set of techniques that leverage probability
distributions and random variables to posit, test and refine hypothesis about
the behaviour of systems. Given observations of a system, the task of probabilistic
modelling normally boils down to finding a probability distribution which:
\begin{itemize}
    \item Is consistent with the observed data;
    \item Is consistent with \textbf{new}, previously unobserved data, originated\
        in the same system.
\end{itemize}

This probability distribution is commonly called the \emph{model}. A good model
will be a good \emph{emulator} of the true generative process that originated
the observed data. In loose terms, this can be summarized as:
\begin{align}
    \mbox{data} \sim p(\mbox{data}|\mbox{hypothesis}^*),
\end{align} where $\text{hypothesis}^*$ is the optimal hypothesis.

Via Bayes' Law, we can write:
\begin{align}
    p(\mbox{hypothesis}|\mbox{data}) = \frac{p(\mbox{data}|\mbox{hypothesis})p(\mbox{hypothesis})}{p(\mbox{data})}
\end{align}

In practice, a modeller will search for an hypothesis that maximizes (some form,
or approximation of) this expression. For simpler problems, this search happens
in closed-form, i.e., there is an expression to compute the optimal hypothesis,
given data. However, for most real-world problems there is no closed-form solution,
and the modeller has to resort to algorithms and approximations, and will only
be able to find \textbf{local} optima for the above expression, in most cases.

It's also worth noting that there are effectively infinite candidate distributions -
each one an hypothesis for how the system at hand generates data. It is common
to make use of domain knowledge and assume the true system has a certain intrinsic
structure and form, and to use these assumptions to constrain the space of
candidate hypothesis. Assumptions about structure usually translate to conditional
independence claims between some or all of the observed variables; assumptions
about form translate into the use of specific parametric families to govern some
or all of the observed variables. These assumptions are commonly connected between
themselves (for example, when conjugate likelihood-prior pairs are used).

When parametric forms are used, an hypothesis is uniquely defined by the set of
parameters it requires - commonly called $\theta$\footnote{For the type of models
and problems dealt with in this work, I will assume $\theta$ is finite, but it's
worth noting that there are models for which the size of $\theta$ \emph{grows}
with the dataset size. These are called non-parametric models. They come with
their own advantages and disadvantages, which are out of the scope of this work.}.

\section{Model Complexity}
\label{section:modelcomplexity}
Intuitively, the size of $\theta$ is deeply connected with the expressiveness
of the distribution. In practice, this translates to the observation that if we
make the model\footnote{Throughout this work I will be using the words \emph{model}
and \emph{distribution} almost interchangeably, making it clear when context isn't
enough.} expressive enough, it can fit the observed data arbitrarily well. Na√Øvely,
this would be a desirable characteristic to exploit - it's always possible to increase
the likelihood by adding parameters to the model. However, increasing model
complexity normally comes at the expense of generalization. This phenomenon is
commonly referred to as \emph{overfitting}, and there are several
angles from which to explain it and interpret it. Namely:
\begin{itemize}
    \item The classical perspective is that of the bias-variance tradeoff. To
        understand this, consider the concept of an Hypothesis Class - a set
        of hypotheses in which, via some procedure, the modeller will search
        for an hypothesis that is consistent with the observed data, and is
        expected to generalize to unseen data. Said procedure is what is normally
        referred to as \emph{fitting} the model to the data. In the case of
        parametric models, the set of models of a given parametric form, with
        a parameter-vector of a certain fixed size, is an example of an Hypothesis
        Class. Intuitively, a more complex Hypothesis Class is more likely to
        contain the true hypothesis (or a good approximation to it). However,
        the more complex the Hypothesis Class, the larger the search-space -
        the higher the number of candidate hypothesis. In this sense, an increase
        in the size of the search-space often translates into an increase of the
        sensitivity to the problem variables (in the case of learning and inference,
        this means sensitivity to initialization and to the data used to fit).
        Conversely, a simpler model will constitute a smaller search-space, hence
        the search procedure will be less sensitive to initalization and problem
        variables. However, the true hypothesis (or a good approximation to it)
        is less likely to be contained in it - precisely because it is a smaller
        Hypothesis Class. The bias-variance tradeoff is a summary of these observations:
        a highly complex model is potentially able to achieve a low expected error
        on observed data (low bias), but will tend to be extremely sensible to small
        variations on its input (high variance). Conversely, a simpler model will
        be more robust to variations on its input (low variance), but won't have the same
        modelling capacity and will produce a larger expected error (high bias). %\footnote{
        %The number of parameters is far from being the best measure of complexity
        %of a model. Nevertheless, it is a good proxy to compare model complexity
        %between models of the same parametric family. However, recent work by
        %Belkin et al. \cite{Belkin2018Dec} shows that modern machine learning
        %contexts, in which the number of parameters is far larger than in classical
        %settings, have to be understood under a measure of model complexity
        %different than the traditional ones. This is because it is now common
        %practice to fit highly overparameterized models to a point of interpolation
        %(close to zero training error), still being able to achieve good generalization.
        %To explain this, the authors of \cite{Belkin2018Dec} 
        %}
    \item Andrey Kolmogorov's and Gregory Chaitin's ideas on Algorithmic Information
        Theory \cite{chaitin-leibniz}, and Kolmogorov complexity  are another
        useful lens through which to regard this question. Consider that data are
        measurements of phenomena. Modelling is concerned with finding the
        laws that explain/govern these phenomena. Intuitively, if the laws are
        as complex as the data they intend to explain, they aren't explaining anything.
        AIT formalizes this notion by borrowing the concept of \emph{program} to
        define the generative process by which observed data comes to existence.
        The complexity of a dataset is then easy to define: it is the size of
        the \textbf{smallest}\footnote{Note the emphasis on "smallest" - this is
        because any program can be made arbitrarily redundant, and thus arbitrarily large.}
        program that generates the observed data. And the appropriate unit with
        which to measure the size of a program - and, as we've now seen, the
        complexity of a dataset - is bits\footnote{Or the basic unit of memory
        of the computer where the data generating program would run}. The parallel
        between these ideas and the question of overfitting is thus easy to make:
        a program (or a model and its parameter vector)  is useful if it 
        \emph{compresses} the data, intuitively because to do so it leverages the
        patterns therein, which are the object of interest in the modelling task.
\end{itemize}

Both of these lines of reasoning make clear that there is a certain balance
in complexity that a good model has to achieve: it should be parsimonious enough
that it won't overfit, but flexible enough that it is able to properly explain
the observed data.  There are strategies to make this mathematically objective.
Some of those methods are the Bayesian Information Criterion, the Akaike
Information Criterion and the Minimum Description Length.

\section{Structure and Latent Variables}
\label{section:probmodellatvar}
In some cases, one might want to leverage some available domain knowledge. This
often translates into assuming that there is some latent structure in the data.
This structure is encoded into latent variables and their influence over the
observable variables.

In this scenario, we become interested in the distribution given by
$p(x, z, \theta_x, \theta_z)$, where $z$ is the latent variable, $\theta_x$ is
the parameter vector for the distribution over $\mathcal{X}$, and $\theta_z$ is
the parameter vector for the distribution over $\mathcal{Z}$.

For structure and latent variables to be useful we normally make the additional
assumption that we have the ability of factorizing that distribution in ways that
make it tractable. If we have a dataset $\mathbf{X} = {x_1, x_2, x_3, ..., x_N}$, with
$N$ i.i.d. samples, and $N$ latent variables, $\mathbf{Z} = {z_1, z_2, z_3, ..., z_N}$,
one common factorization is:
\begin{align}
    p(\mathbf{X}, \mathbf{Z}, \theta) = \Big(\prod^N_{i=1} p(x_i| z_i, \theta_x) p(z_i | \theta_z)\Big) p(\theta_x) p(\theta_z). \label{eq:latentvariable}
\end{align}

It's also possible that the samples of $\mathbf{X}$ have some sort of causal
relation, for instance if they occur ordered in time. In this case, they are not i.i.d.
One way to encode this assumption is to posit an \emph{autoregressive} model, i.e.,
a model in which a random variable depends on the variables that come before it.
If each random variable depends solely on the random variable that precedes it,
this is called a Markov Model. A common variation of the Markov Model is the
Hidden Markov Model, where the autoregressive part of the model is present only
in the latent variables:
\begin{align}
    p(\mathbf{X}, \mathbf{Z}, \theta) = p(z_1) \Big(\prod^N_{i=2} p(x_i | z_i, \theta_x) p(z_i| z_{i-1}, \theta_z) \Big) p(\theta_x) p(\theta_z).
\end{align}

These are merely examples of models with different structure assumptions encoded
into them. Normally, if the structure has a certain regularity, it's possible
to exploit it to obtain tractable (approximate) inference and estimation methods.

\section{Mixture Models}
Mixture Models are a subset of the structure "family" described in \ref{eq:latentvariable},
and they have a central role in this work.

In a Mixture Model there is a discrete latent variable $z_i$ which selects
one of $K$ components from which an observation $x_i$ will be sampled. This
can be summarized as:
\begin{align}
    z_i \sim p(z_i | \pi) \\
    x_i \sim p(x_i | z_i)
\end{align}

It's common to assume that all of the $K$ components are part of the same
parametric family. In that case, we can rewrite the above as:
\begin{align}
    z_i \sim p(z_i | \pi) \\
    x_i \sim p(x_i | \theta_{z_i}),
\end{align} where it is made evident that the discrete variable $z_i$ is selecting
the \textbf{parameter vector} to be used for sample $x_i$.

The most discussed mixture model is the Gaussian Mixture Model, in which the $K$
components of the model are Gaussian distributions.

\section{Approximate Inference}
\label{section:probmodelinf}
Take the expression $p(x, z, \theta)$. For simplicity, let us consider $\theta$
as part of the latent variables $z$. This means that the model is simply written
as the joint distribution: $p(x, z)$. \emph{Inference}\footnote{If we hadn't collapsed
$\theta$ into $z$ and were instead handling separately, we would call \textbf{inference}
to the task of finding $z$ and \textbf{learning} to the task of finding $\theta$}
is the task of finding the most probable $z$ after having observed $x$ . Specifically,
the goal is to find the posterior distribution of $z$, given $x$, i.e.: $p(z|x)$.

Recall Bayes' Law:

\begin{align}
    p(z|x) &= \frac{p(x|z)p(z)}{p(x)} \\
           &= \frac{p(x|z)p(z)}{\int p(x|z')p(z') dz'}
\end{align}

For the vast majority of cases, the integral on the denominator will be
intractable. To overcome this difficulty we normal resort to two families
of methods: Monte-Carlo methods, and Variational methods.

\subsection{Monte-Carlo Methods}
\label{subsection:mcmc}

Monte-Carlo methods work by using sampling techniques to approximate the
intractable integral. The most powerful subclass of these methods is called
Markov-chain Monte-Carlo. Its approach consists of devising a scheme that
allows for sampling from a distribution close to the one of interest. It
accomplishes this by defining a Markov-Chain whose transition function 
is guaranteed to make it converge asymptotically to the distribution of interest,
given some constraints (ergodicity...) (TODO: explain more)

\subsection{Variational Methods}
\label{subsection:variational}
Variational methods work by turning the problem of integration into one of
optimization. They propose a family of parametric distributions, and then
optimize the parameters so as to minimize the "distance" between the approximate
(normally called "variational") distribution and the distribution of interest.

There are two ways to derive the most commonly used objective function for
this problem, which will be detailed in the two following subsections.

\subsubsection{Kullback-Leibler Divergence}
\label{subsubsection:kldiv}

The Kullback-Leibler divergence is a measure\footnote{Note that the KL divergence
isn't symmetric and as such I haven't called it a \emph{metric}} of the distance
between two probability distributions $p$, and $q$. It is given by:

\begin{align}
    KL(q||p) = \int q \log\frac{q}{p}
\end{align}

In the setting of inference, $p$ is the posterior $p(z|x)$ and $q$ is a distribution
in some parametric family, with parameters $\phi$, i.e., $q(z; \phi)$. However,
it's clear that we can't compute the Kullback-Leibler directly, because it
requires the knowledge of both the distributions, and finding $p(z|x)$ is precisely
the task at hand. Let us expand the KL divergence expression:

\begin{align}
    KL(q||p) &= \int q(z) (\log q(x) - \log p(z|x)) dz \\
             &= \int q(z) (\log q(z) - (\log p(x, z) - \log p(x))) dz \\
             &= \mathbb{E}_q [\log q(z)] - \mathbb{E}_q [\log p(x, z)] + \mathbb{E}_q [\log p(x)] \\
             &= \mathbb{E}_q [\log q(z)] - \mathbb{E}_q [\log p(x, z)] + \log p(x) \\
\end{align}

The last term is constant w.r.t $q(z)$. In that sense, for a fixed $p(x)$,
minimizing the KL divergence is equivalent to minimizing
\begin{align}
    \mathbb{E}_q [\log q(z)] - \mathbb{E}_q [\log p(x, z)],
\end{align} which is equivalent to maximizing
\begin{align}
    \mathbb{E}_q [\log p(x, z)] - \mathbb{E}_q [\log q(z)]. \label{eq:elbokldiv}
\end{align} This quantity is commonly refered to as ELBO - Expectation Lower BOund.
It can be rewritten as:

\begin{align}
    ELBO(q) &= \mathbb{E}_q [\log p(x, z)] - \mathbb{E}_q [\log q(z)] \\
            &= \mathbb{E}_q [\log p(x|z)] + \mathbb{E}_q [\log p(z)] - \mathbb{E}_q [\log q(z)]
\end{align}

In this form, each term of the ELBO has an easily interpretable role:
\begin{itemize}
    \item $\mathbb{E}_q [\log p(x|z)]$ tries to maximize the conditional likelihood of $x$. That
        can be seen as assigning high probability mass to values of $z$ that \emph{explain} $x$
        well.
    \item $\mathbb{E}_q [\log p(z)]$ is the symmetric of the crossentropy between
        $q(z)$ and $p(z)$. Maximizing this quantity is equivalent of minimizing
        that crossentropy. This can be regarded as a regularizer that discourages
        $q(z)$ of being too different from the prior $p(z)$.
    \item $ - \mathbb{E}_q [\log q(z)]$ is the entropy of $q(z)$. Maximizing
        this term incentivizes the probability mass of $q(z)$ to be spread out:
        another form of regularization.
\end{itemize}

\subsubsection{A lower bound on $\log p(x)$}
\label{subsubsection:elbo}

Another way of approaching the intractable posterior is to start by stating
that our inherent goal is to maximize $p(x)$, or equivalently $\log p(x)$. Given
that, consider the following:
\begin{align}
    \log p(x) &= \log \int p(x, z) dz\\
    &= \log \int q(z) \frac{p(x, z)}{q(z)} dz \\
    &= \log \mathbb{E}_q[\frac{p(x, z)}{q(z)}] \label{eq:elbojensen1} \\
    &\geq \mathbb{E}_q[\log \frac{p(x, z)}{q(z)}] \label{eq:elbojensen2} \\
    &\geq \mathbb{E}_q[\log p(x, z)] - \mathbb{E}_q[q(z)] \label{eq:elbojensen3}
\end{align}

To understand this derivation, consider Jensen's inequality, given (in one of
its forms) by:
\begin{align}
    \phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)], \label{eq:jensen}
\end{align} where $\phi(.)$ is a convex function.

If $\xi(.)$ is a concave function, then $- \xi(.)$ is a convex function, and we
obtain the reverse inequality (substituting $\phi(.)$ with $-\xi(.)$ in the
inequality in \ref{eq:jensen}):
\begin{align}
    -\xi(\mathbb{E}[X]) &\leq \mathbb{E}[-\xi(X)] \\
    \xi(\mathbb{E}[X]) &\geq \mathbb{E}[\xi(X)]
\end{align}

This form is the most useful for us, since $\log$ is a concave function. Using
this, the step between \ref{eq:elbojensen1} and \ref{eq:elbojensen2} is made
obvious.

Note that the right-hand side of \ref{eq:elbojensen3} is the same quantity
we arrived at in \ref{eq:elbokldiv}, and that it is a lower-bound on the
quantity we want to maximize, and so we want to maximize it. It's worth noting
that when $q(z) = p(z|x)$, the bound is tight.
