\chapter{Probabilistic Modelling}
\label{chapter:probmodel}

\section{Introduction}
\label{section:probmodelintro}
Probabilistic modelling is a set of techniques that leverage probability
distributions and random variables to posit, test and refine hypothesis about
the behaviour of systems. Given observations of a system, the task of probabilistic
modelling normally boils down to finding a probability distribution which:
\begin{itemize}
    \item Is consistent with the observed data;
    \item Is consistent with \textbf{new}, previously unobserved data, originated\
        in the same system.
\end{itemize}

This probability distribution is commonly called the \emph{model}. A good model
will be a good \emph{emulator} of the true generative process that originated
the observed data. In loose terms, this can be summarized as:
\begin{align}
    \mbox{data} \sim p(\mbox{data}|\mbox{hypothesis}^*),
\end{align} where $\text{hypothesis}^*$ is the optimal hypothesis.

Via Bayes' Law, we can write:
\begin{align}
    p(\mbox{hypothesis}|\mbox{data}) = \frac{p(\mbox{data}|\mbox{hypothesis})p(\mbox{hypothesis})}{p(\mbox{data})} \label{eq:bayes}
\end{align}

In practice, a modeller will search for an hypothesis that maximizes (some form,
or approximation of) this expression. For simpler problems, this search happens
in closed-form, i.e., there is an expression to compute the optimal hypothesis,
given data. However, for most real-world problems there is no closed-form solution,
and the modeller has to resort to algorithms and approximations, and will only
be able to find \textbf{local} optima for the above expression, in most cases.

It's also worth noting that there are effectively infinite candidate distributions -
each one an hypothesis for how the system at hand generates data. It is common
to make use of domain knowledge and assume the true system has a certain intrinsic
structure and form, and to use these assumptions to constrain the space of
candidate hypothesis. Assumptions about structure usually translate to conditional
independence claims between some or all of the observed variables; assumptions
about form translate into the use of specific parametric families to govern some
or all of the observed variables. These assumptions are commonly connected between
themselves (for example, when conjugate likelihood-prior pairs are used).

When parametric forms are used, an hypothesis is uniquely defined by the set of
parameters it requires - commonly called $\theta$\footnote{For the type of models
and problems dealt with in this work, I will assume $\theta$ is finite, but it's
worth noting that there are models for which the dimension of $\theta$ \emph{grows}
with the dataset size. These are called non-parametric models. They come with
their own advantages and disadvantages, which are out of the scope of this work.}.

\section{Model Complexity}
\label{section:modelcomplexity}
Intuitively, the dimension of $\theta$ is deeply connected with the expressiveness
of the distribution. In practice, this translates to the observation that if we
make the model\footnote{Throughout this work I will be using the words \emph{model}
and \emph{distribution} almost interchangeably, making it clear when context isn't
enough.} expressive enough, it can fit the observed data arbitrarily well. Naïvely,
this would be a desirable characteristic to exploit - it's always possible to increase
the likelihood by adding parameters to the model. However, increasing model
complexity normally comes at the expense of generalization. This phenomenon is
commonly referred to as \emph{overfitting}, and there are several
angles from which to explain it and interpret it. Namely:
\begin{itemize}
    \item The classical perspective is that of the bias-variance tradeoff. To
        understand this, consider the concept of an Hypothesis Class - a set
        of hypotheses in which, via some procedure, the modeller will search
        for an hypothesis that is consistent with the observed data, and is
        expected to generalize to unseen data. Said procedure is what is normally
        referred to as \emph{fitting} the model to the data. In the case of
        parametric models, the set of models of a given parametric form, with
        a parameter-vector of a certain fixed dimension, is an example of an Hypothesis
        Class. Intuitively, a more complex Hypothesis Class is more likely to
        contain the true hypothesis (or a good approximation to it). However,
        the more complex the Hypothesis Class, the larger the search-space -
        the higher the number of candidate hypothesis. In this sense, an increase
        in the size of the search-space often translates into an increase of the
        sensitivity to the problem variables (in the case of learning and inference,
        this means sensitivity to initialization and to the data used to fit).
        Conversely, a simpler model will constitute a smaller search-space, hence
        the search procedure will be less sensitive to initalization and problem
        variables. However, the true hypothesis (or a good approximation to it)
        is less likely to be contained in it - precisely because it is a smaller
        Hypothesis Class. The bias-variance tradeoff is a summary of these observations:
        a highly complex model is potentially able to achieve a low expected error
        on observed data (low bias), but will tend to be extremely sensible to small
        variations on its input (high variance). Conversely, a simpler model will
        be more robust to variations on its input (low variance), but won't have the same
        modelling capacity and will produce a larger expected error (high bias).\footnote{
        The number of parameters is far from being the best measure of complexity
        of a model. Nevertheless, it is a good proxy to compare model complexity
        between models of the same parametric family. However, recent work by
        Belkin et al. \cite{Belkin2018Dec} shows that modern machine learning
        contexts, in which the number of parameters is far larger than in classical
        settings, have to be understood under a measure of model complexity
        different than the traditional ones. This is because it is now common
        practice to fit highly overparameterized models to a point of interpolation
        (close to zero training error), still being able to achieve good generalization.}
    \item Andrey Kolmogorov's and Gregory Chaitin's ideas on Algorithmic Information
        Theory \cite{chaitin-leibniz}, and Kolmogorov complexity  are another
        useful lens through which to regard this question. Consider that data are
        measurements of phenomena. Modelling is concerned with finding the
        laws that explain/govern these phenomena. Intuitively, if the laws are
        as complex as the data they intend to explain, they aren't explaining anything.
        AIT formalizes this notion by borrowing the concept of \emph{program} to
        define the generative process by which observed data comes to existence.
        The complexity of a dataset is then easy to define: it is the size of
        the \textbf{smallest}\footnote{Note the emphasis on "smallest" - this is
        because any program can be made arbitrarily redundant, and thus arbitrarily large.}
        program that generates the observed data. And the appropriate unit with
        which to measure the size of a program - and, as we've now seen, the
        complexity of a dataset - is bits\footnote{Or the basic unit of memory
        of the computer where the data generating program would run}. The parallel
        between these ideas and the question of overfitting is thus easy to make:
        a program (or a model and its parameter vector)  is useful if it 
        \emph{compresses} the data, intuitively because to do so it leverages the
        patterns therein, which are the object of interest in the modelling task.
\end{itemize}

Both of these lines of reasoning make clear that there is a certain balance
in complexity that a good model has to achieve: it should be parsimonious enough
that it won't overfit, but flexible enough that it is able to properly explain
the observed data.  There are strategies to make this mathematically objective.
Some of those methods are the Bayesian Information Criterion, the Akaike
Information Criterion and the Minimum Description Length.

\section{MAP and ML Estimation}

Once the parametric form of the model is defined, the task at hand becomes the
discovery of the parameter vector $\theta$ that best explains the observed
data, within the defined parametric family.

The naïve (but often the only possible) approach is to maximize what is called
the likelihood function, given by:
\begin{align}
    \mathcal{L}(\theta) = p(x | \theta)
\end{align}

This approach is called \emph{Maximum Likelihood Estimation}. Note that $\mathcal{L}$
is a function of $\theta$. Depending on the model, finding $\theta^*$ - the optimum -
can be as simple as computing an analytical expression, or as difficult as using
gradient-based methods to optimize a non-convex objective. In the latter case, a
local optima is usually the best one can expect to obtain.

Another approach, called \emph{Maximum a posteriori}, works by retrieving the
mode of the posterior distribution of the parameter-vector, given by:
\begin{align}
    p(\theta | x) = \frac{p(x | \theta)p(\theta)}{p(x)}
\end{align}

It's easy to see that these two approaches are intimately related. MAP differs
from ML by the fact that it employs the prior $p(\theta)$ to give different
weights to different hypothesis (i.e., different instances of $\theta$). This
is useful if there is domain knowledge available that can be encoded in the
prior.

It is worth noting that ML is a special case of MAP, when the prior is uniform.

\section{Structure and Latent Variables}
\label{section:probmodellatvar}
In some cases, one might want to leverage some available domain knowledge. This
often translates into assuming that there is some latent structure in the data.
This structure is commonly encoded into latent variables and their influence over
the observable variables.

In this scenario, we become interested in the distribution given by
$p(x, z, \theta_x, \theta_z)$, where $z$ is the latent variable, $\theta_x$ is
the parameter vector for the distribution over $\mathcal{X}$, and $\theta_z$ is
the parameter vector for the distribution over $\mathcal{Z}$.

For structure and latent variables to be useful we normally make the additional
assumption that we have the ability of factorizing their joint distribution in ways that
make it tractable. If we have a dataset $\mathbf{X}$, with $N$ i.i.d. samples $x_1, x_2, x_3, ..., x_N$
and $N$ latent variables $z_1, z_2, z_3, ..., z_N$, one common factorization is:
\begin{align}
    p(\mathbf{X}, \mathbf{Z}, \theta) = \Big(\prod^N_{i=1} p(x_i| z_i, \theta_x) p(z_i | \theta_z)\Big) p(\theta_x) p(\theta_z). \label{eq:latentvariable}
\end{align}

It's also possible that the samples in $\mathbf{X}$ have some sort of causal
relation, for instance if they occur ordered in time. In this case, they are not i.i.d.
One way to encode this assumption is to posit an \emph{autoregressive} model, i.e.,
a model in which a random variable depends on the variables that come before it.
If each random variable depends solely on the random variable that precedes it,
this is called a Markov Model. A common variation of the Markov Model is the
Hidden Markov Model, where the autoregressive part of the model is present only
in the latent variables:
\begin{align}
    p(\mathbf{X}, \mathbf{Z}, \theta) = p(z_1) \Big(\prod^N_{i=2} p(x_i | z_i, \theta_x) p(z_i| z_{i-1}, \theta_z) \Big) p(\theta_x) p(\theta_z).
\end{align}

These are merely examples of models with different structure assumptions encoded
into them. Normally, if the structure has a certain regularity, it's possible
to exploit it to obtain tractable (approximate) inference and estimation methods.

\section{Mixture Models and the EM Algorithm}
Mixture Models are a subset of the structure "family" described in \ref{eq:latentvariable},
and they have a central role in this work.

In a Mixture Model there is a discrete latent variable $z_i$ which selects
one of $K$ components from which an observation $x_i$ will be sampled. This
can be summarized as:
\begin{align}
    z_i \sim p(z_i | \pi) \\
    x_i \sim p(x_i | z_i)
\end{align}

The probability of $x_i$ being sampled from component $k$ (that is, the proability
of $z_i = k$) is commonly referred to as the weight of component $k$.

It's common to assume that all of the $K$ components are part of the same
parametric family. In that case, we can rewrite the above as:
\begin{align}
    z_i \sim p(z_i | \pi) \\
    x_i \sim p(x_i | \theta_{z_i}),
\end{align} where it is made evident that the discrete variable $z_i$ is selecting
the \textbf{parameter vector} to be used for sample $x_i$.

The most discussed mixture model is the Gaussian Mixture Model, in which the $K$
components of the model are Gaussian distributions.

\subsubsection{The EM Algorithm}
The Expectation-Maximization algorithm is the most commonly used algorithm to
fit Mixture Models\footnote{Although it can be used to fit other types of models.}.

The starting point of EM is the realisation that if all variables were observed,
it would be easy to apply ML or MAP estimation for the parameters. Given that,
the algorithm can be generally described as an alternation between two steps:
\begin{itemize}
    \item \textbf{E-step}: infer the most probable values of the unobserved variables.
        (In the case of Mixture Models, this corresponds to inferring the discrete
        variables that select the component from which each observed data point
        was sampled. This can be roughly thought of as a cluster assignment).
    \item \textbf{M-step}: given the observed variables and the values inferred
        on the previous step, optimize the model parameters. (In the case of 
        Mixture Models, this correponds to inferring the parameters of each
        component, and its weight).
\end{itemize}

A more rigorous description of the procedure follows. Consider the following
expression:
\begin{align}
    \ell_c(\theta) = \sum^N_{i=1} \log p(x_i, z_i | \theta)
\end{align}

This is the complete data log-likelihood. Like the likelihood function, it is a
function of $\theta$, which is easy to compute, given all the $x_i$ and $z_i$.
However, the $z_i$ aren't observed.

To overcome this, let us define the expected complete data log likelihood:
\begin{align}
    Q(\theta, \theta^{(t-1)}) = \mathbb{E}_{Z|X, \theta^{(t-1)}}[\ell_c(\theta) | X, Z, \theta^{(t-1)}],
\end{align} where $\theta^{(t)}$ represents the value of $\theta$ at time step
$t$ of the fitting procedure.

Note that the expectation is w.r.t. $Z$, given $X$ and $\theta^{(t-1)}$. It is the
expected value of $\ell_c(\theta)$, given the parameter values obtained at the
previous step of the the algorithm. Depending on the nature of $Z$, this expectation
can be obtained either in closed form or approximated, for instance via samples
of $z_i$ (if a sampling procedure is available).

In the case of Mixture Models, where the $z_i$ are instances of a categorical
random variable, the expression for $Q(\theta, \theta^{(t-1)})$ can be made
simpler as such:
\begin{align}
    Q(\theta, \theta^{(t-1)}) &= \mathbb{E}_{Z|X, \theta^{(t-1)}}[\ell_c(\theta) | X, Z, \theta^{(t-1)}] \\
    &= \mathbb{E}_{Z|X, \theta^{(t-1)}}[\sum_{i=1}^N \log p(x_i, z_i | \theta)] \\
    &= \mathbb{E}_{Z|X, \theta^{(t-1)}}[\sum_{i=1}^N \log \prod_{k=1}^K \pi_k p(x_i | \theta_k)^{\mathbb{I}(z_i = k)}] \\
    &= \mathbb{E}_{Z|X, \theta^{(t-1)}}[\sum_{i=1}^N \sum_{k=1}^K \log\Big(\pi_k p(x_i | \theta_k)^{\mathbb{I}(z_i = k)}\Big)] \\
    &= \mathbb{E}_{Z|X, \theta^{(t-1)}}[\sum_{i=1}^N \sum_{k=1}^K \mathbb{I}(z_i = k) \log\Big(\pi_k p(x_i | \theta_k)\Big)] \\
    &= \sum_{i=1}^N \sum_{k=1}^K \underbrace{\mathbb{E}_{Z|X, \theta^{(t-1)}}[\mathbb{I}(z_i = k)]}_{\text{Let this quantity be represented by $r_{ik}$}} \log\Big(\pi_k p(x_i | \theta_k)\Big)\\
    &= \sum_{i=1}^N \sum_{k=1}^K r_{ik} \log\Big(\pi_k p(x_i | \theta_k)\Big) \\
    &= \sum_{i=1}^N \sum_{k=1}^K r_{ik} \log\pi_k + \sum_{i=1}^N \sum_{k=1}^K r_{ik} \log p(x_i | \theta_k) \\
\end{align}

The quantity defined above as $r_{ik}$ is referred to as the responsability. It
is trivial to arrive at a closed-form expression for it, given the value of $\theta$
arrived at the previous iteration, i.e. $\theta^{(t-1)}$:
\begin{align}
    r_{ik} &= p(z_i = k | x_i\ ; \theta^{(t-1)}) \\
    &= \frac{p(z_i = k , x_i\ ; \theta^{(t-1)})}{p(x_i\ ; \theta^{(t-1)})} \\
    &= \frac{p(z_i = k , x_i\ ; \theta^{(t-1)})}{p(x_i\ ; \theta^{(t-1)})} \\
    &= \frac{p(z_i = k , x_i\ ; \theta^{(t-1)})}{\sum_{k'} p(z_i = k' , x_i\ ; \theta^{(t-1)})} \\
    &= \frac{\pi_k p(x_i | \theta_k^{(t-1)})}{\sum_{k'} \pi_{k'} p(x_i | \theta_{k'}^{(t-1)})}
\end{align}



The EM algorithm for Mixture Models then becomes an alternation between the
following two steps:
\begin{itemize}
    \item \textbf{E-step}: Compute the responsabilies $r_{ik}$ for each $x_i$.
    \item \textbf{M-step}: Given $r_{ik}$, solve the following optimization
        problem:
        \begin{align}
            \theta^{(t)} = \argmax_\theta Q(\theta, \theta^{(t-1)})
                \underbrace{+ \log p(\theta)}_\text{Optional, if we want to do MAP estimation},
        \end{align} which can have a closed-form solution in some of the simplest Mixture Models,
        like Gaussian Mixture Models, but can require a gradient-based optimization procedure for
        more flexible models.
\end{itemize}

\section{Approximate Inference}
\label{section:probmodelinf}
Take the expression $p(x, z, \theta)$. For simplicity, let us consider $\theta$
as part of the latent variables $z$. This means that the model is simply written
as the joint distribution: $p(x, z)$. \emph{Inference}\footnote{If we hadn't collapsed
$\theta$ into $z$ and were instead handling separately, we would call \textbf{inference}
to the task of finding $z$ and \textbf{learning} to the task of finding $\theta$}
is the task of finding the most probable $z$ after having observed $x$ . Specifically,
the goal is to find the posterior distribution of $z$, given $x$, i.e.: $p(z|x)$.

Recall Bayes' Law:

\begin{align}
    p(z|x) &= \frac{p(x|z)p(z)}{p(x)} \\
           &= \frac{p(x|z)p(z)}{\int p(x|z')p(z') dz'}
\end{align}

For the vast majority of cases, the integral on the denominator will be
intractable. To overcome this difficulty we normal resort to two families
of methods: Monte-Carlo methods - which are out of the scope of this work -
and Variational methods.
%
%\subsection{Monte-Carlo Methods}
%\label{subsection:mcmc}
%
%Monte-Carlo methods work by using sampling techniques to approximate the
%intractable integral. The most powerful subclass of these methods is called
%Markov-chain Monte-Carlo. Its approach consists of devising a scheme that
%allows for sampling from a distribution close to the one of interest. It
%accomplishes this by defining a Markov-Chain whose transition function 
%is guaranteed to make it converge asymptotically to the distribution of interest,
%given some constraints (ergodicity...) (TODO: explain more)

\subsection{Variational Methods}
\label{subsection:variational}
Variational methods work by turning the problem of integration into one of
optimization. They propose a family of parametric distributions, and then
optimize the parameters so as to minimize the "distance" between the approximate
(normally called "variational") distribution and the distribution of interest.

There are two ways to derive the most commonly used objective function for
this problem, which will be detailed in the two following subsections.

\subsubsection{Kullback-Leibler Divergence}
\label{subsubsection:kldiv}

The Kullback-Leibler divergence is a measure\footnote{Note that the KL divergence
isn't symmetric and as such I haven't called it a \emph{metric}} of the distance
between two probability distributions $p$, and $q$. It is given by:

\begin{align}
    KL(q||p) = \int q \log\frac{q}{p}
\end{align}

In the setting of inference, $p$ is the posterior $p(z|x)$ and $q$ is a distribution
in some parametric family, with parameters $\phi$, i.e., $q(z; \phi)$. However,
it's clear that we can't compute the Kullback-Leibler directly, because it
requires the knowledge of both the distributions, and finding $p(z|x)$ is precisely
the task at hand. Let us expand the KL divergence expression:

\begin{align}
    KL(q||p) &= \int q(z) (\log q(x) - \log p(z|x)) dz \\
             &= \int q(z) (\log q(z) - (\log p(x, z) - \log p(x))) dz \\
             &= \mathbb{E}_q [\log q(z)] - \mathbb{E}_q [\log p(x, z)] + \mathbb{E}_q [\log p(x)] \\
             &= \mathbb{E}_q [\log q(z)] - \mathbb{E}_q [\log p(x, z)] + \log p(x) \\
\end{align}

The last term is constant w.r.t $q(z)$. In that sense, for a fixed $p(x)$,
minimizing the KL divergence is equivalent to minimizing
\begin{align}
    \mathbb{E}_q [\log q(z)] - \mathbb{E}_q [\log p(x, z)],
\end{align} which is equivalent to maximizing
\begin{align}
    \mathbb{E}_q [\log p(x, z)] - \mathbb{E}_q [\log q(z)]. \label{eq:elbokldiv}
\end{align} This quantity is commonly refered to as ELBO - Evidence Lower BOund.
It can be rewritten as:

\begin{align}
    ELBO(q) &= \mathbb{E}_q [\log p(x, z)] - \mathbb{E}_q [\log q(z)] \\
            &= \mathbb{E}_q [\log p(x|z)] + \mathbb{E}_q [\log p(z)] - \mathbb{E}_q [\log q(z)]
\end{align}

In this form, each term of the ELBO has an easily interpretable role:
\begin{itemize}
    \item $\mathbb{E}_q [\log p(x|z)]$ tries to maximize the conditional likelihood of $x$. That
        can be seen as assigning high probability mass to values of $z$ that \emph{explain} $x$
        well.
    \item $\mathbb{E}_q [\log p(z)]$ is the symmetric of the crossentropy between
        $q(z)$ and $p(z)$. Maximizing this quantity is equivalent of minimizing
        that crossentropy. This can be regarded as a regularizer that discourages
        $q(z)$ of being too different from the prior $p(z)$.
    \item $ - \mathbb{E}_q [\log q(z)]$ is the entropy of $q(z)$. Maximizing
        this term incentivizes the probability mass of $q(z)$ to be spread out:
        another form of regularization.
\end{itemize}

\subsubsection{A lower bound on $\log p(x)$}
\label{subsubsection:elbo}

Another way of approaching the intractable posterior is to start by stating
that our inherent goal is to maximize $p(x)$, or equivalently $\log p(x)$. Given
that, consider the following:
\begin{align}
    \log p(x) &= \log \int p(x, z) dz\\
    &= \log \int q(z) \frac{p(x, z)}{q(z)} dz \\
    &= \log \mathbb{E}_q[\frac{p(x, z)}{q(z)}] \label{eq:elbojensen1} \\
    &\geq \mathbb{E}_q[\log \frac{p(x, z)}{q(z)}] \label{eq:elbojensen2} \\
    &\geq \mathbb{E}_q[\log p(x, z)] - \mathbb{E}_q[q(z)] \label{eq:elbojensen3}
\end{align}

To understand this derivation, consider Jensen's inequality, given (in one of
its forms) by:
\begin{align}
    \phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)], \label{eq:jensen}
\end{align} where $\phi(.)$ is a convex function.

If $\xi(.)$ is a concave function, then $- \xi(.)$ is a convex function, and we
obtain the reverse inequality (substituting $\phi(.)$ with $-\xi(.)$ in the
inequality in \ref{eq:jensen}):
\begin{align}
    -\xi(\mathbb{E}[X]) &\leq \mathbb{E}[-\xi(X)] \\
    \xi(\mathbb{E}[X]) &\geq \mathbb{E}[\xi(X)]
\end{align}

This form is the most useful for us, since $\log$ is a concave function. Using
this, the step between \ref{eq:elbojensen1} and \ref{eq:elbojensen2} is made
obvious.

Note that the right-hand side of \ref{eq:elbojensen3} is the same quantity
we arrived at in \ref{eq:elbokldiv}, and that it is a lower-bound on the
quantity we want to maximize, and so we want to maximize it. It's worth noting
that when $q(z) = p(z|x)$, the bound is tight.
