\chapter{Experiments}
\label{chapter:experiments}

In this chapter, the proposed model is applied to two synthetic datasets (Pinwheel
and Two-circles) and one real-world dataset (MNIST). On one of the synthetic
datasets, one shortcoming of the model is brought to attention, but is overcome
in a semi-supervised setting. On the real-world dataset, the model's clustering
capabilities are evaluted, as well as its capacity to model complex distributions.

All experiments were conducted using RealNVP (the model proposed in \autocite{real-nvp})
as the normalizing flow model for the mixture components. Moreover, a technique
inspired in \autocite{mixae} was employed to improve training speed and quality
of results. This consisted of dividing the inputs of the softmax layer in
the variational posterior by a temperature value, $t$, which was made to follow
an exponential decay schedule during training. Intuitively, this makes the
variational posterior \q{more certain} as training moves on, while allowing all
components to be generally exposed to the whole data, in the initial epochs.
This incentivizes components to not be \q{subtrained} in the initial epochs and
then prematurely discarded by the variational posterior.

\section{Toy datasets}
\subsection{Pinwheel dataset}

This dataset is constituted by five non-linear \q{wings}. See figure \ref{fig:pinwheel}
for the results of running the model on this dataset. As expected, the variational
posterior has learned to partition the space so as to attribute each \q{wing} to
a component of the mixture. This partitioning is imperfect in regions of space
that have low probability for every component.

\begin{figure}[!htb]
  \begin{subfigmatrix}{2}
    \subfigure[]{\includegraphics[width=0.49\linewidth]{figures/original_pinwheel.png}}
    \subfigure[]{\includegraphics[width=0.49\linewidth]{figures/trained_pinwheel.png}}
  \end{subfigmatrix}
    \caption{(a) Original dataset. (b) Samples from the learned model. Each
dot is colored according to the component it was sampled from. The background
colors denote the regions where each component has maximum probability assigned
by the variational posterior. (Note that the background colors were chosen
so as to not match the dot colors, otherwise it dots wouldn't be visible)}
  \label{fig:pinwheel}
\end{figure}

This experiment consisted of training on 2560 data points, 512 per class; using
the Adam (\autocite{adam}) optimizer, with a learning rate of 0.001, with a
mini-batch size of 512, during 400 epochs. The variational posterior was parameterized
by a multi-layer perceptron, with 1 hidden layer of dimension 3, and with a
softmax output. Each component of the mixture was a RealNVP with 8 blocks, each
block with multi-layer perceptrons, with 1 hidden layer of dimension 8, as the
$s(.)$ and $t(.)$ functions of the affine coupling layers.

\subsection{Two-circles dataset}
This dataset consists of two concentric circles. The experiment on this dataset,
visible on figure \ref{fig:twocircles}, makes evident one shortcoming of this
model: the way in which the variational posterior partitions space is
not necessarily guided by the intrisic structure in the data. In the case of
the two-circles dataset, it was found that the most common space partitioning
induced by the model consisted simply of splitting space in half. However, in
a semi-supervised setting, this behaviour can be corrected and the model
successfully learns to separate the two circles, as is visible in figure
\ref{fig:twocircles-semisup}. In this setting, the model was pretrained on
the labeled instances for some epochs and then trained with the normal procedure.
In the semi-supervised setting the model has the chance to refine both the
variational posterior and each of the components, thus making better use of
the unlabeled data in the unsupervised phase of the training. As is clearly
visible in figure \ref{fig:twocircles-semisup}, the model struggles with
learning full, closed, circles. This is because it is unable to \q{pierce a hole}
in the base distribution, due to the nature of the transformations that are
applicable. Thus, to model a circle, the model has to learn to stretch the blob
formed by the base distribution, and \q{bend it over itself}. This difficulty
is also what keeps the model from learning a structurally interesting solution
in the fully unsupervised case: it is easier to learn to distort space so as to
learn a multimodal distribution that models half of the two circles.

The unsupervised learning experiment consisted of training on 1024 datapoints,
512 per class; using the Adam (\autocite{adam}) optimizer, with a learning rate
of 0.001, with a mini-batch size of 128, during 500 epochs.
The semi-supervised learning experiment consisted of training on 1024 unlabeled
datapoints, 512 per class; and 32 labeled data points, 16 per class. The model
was first pretrained during 300 epochs solely on the 32 labeled data points, using
the labels to selectively optimize each component of the mixture, as well as
to optimize the variational posterior by minimizing a binary cross-entropy loss.
After pretraining, the model was trained by interweaving supervised epochs - like
in pretraining - with unsupervised epochs. Optimization was carried out using the
Adam (\autocite{adam}) optimizer, with a learning rate of 0.001, with a mini-batch size
of 128, during 500 epochs.
For both the unsupervised and the semi-supervised experiments, the neural network
used to parameterize the variational posterior was a multi-layer perceptron, with
2 hidden layers of dimension 16, and with a softmax output. Each component of the
mixture was a RealNVP with 10 blocks, each block with multi-layer perceptrons,
with 1 hidden layer of dimension 8, as the $s(.)$ and $t(.)$ functions of the
affine coupling layers.

\begin{figure}[!htb]
  \begin{subfigmatrix}{2}
    \subfigure[]{\includegraphics[width=0.49\linewidth]{figures/original_2_circles.png}}
    \subfigure[]{\includegraphics[width=0.49\linewidth]{figures/trained_2_circles.png}}
  \end{subfigmatrix}
    \caption{(a) Original dataset. (b) Samples from the learned model, without
    any labels. Coloring logic is the same as in \ref{fig:pinwheel}. (c) Samples
    from semi-supervised scenario.}
\label{fig:twocircles}
\end{figure}

\begin{figure}[!htb]
  \begin{subfigmatrix}{2}
    \subfigure[]{\includegraphics[width=0.49\linewidth]{figures/labeled_2_circles.png}}
    \subfigure[]{\includegraphics[width=0.49\linewidth]{figures/trained_2_circles_semisup.png}}
  \end{subfigmatrix}
    \caption{(a) Labeled points used in semi-supervised scenario. (b) Samples
    from the model trained in the semi-supervised scenario.}
\label{fig:twocircles-semisup}
\end{figure}

\section{Real-world dataset}
In this section, the proposed model is evaluated in the well-known MNIST
dataset(\autocite{MNIST}). This dataset consists of pixel-matrices of images of
handwritten digits. The grids are of dimension 28 x 28, and were flattened
to vectors of dimension 784 for training. For this experiment, only the images
corresponding to the digits from 0 to 4 were considered. The normalizing
flow model used for the components was a MAF, with 5 blocks, whose internal MADE
layers had 1 hidden layer of dimension 200. The variational posterior was parameterized
by a multi-layer perceptron, with 1 hidden layer of dimension 512. The model
was trained for 100 epochs, with a mini-batch size of 100. The Adam optimizer
was used, with a learning rate of 0.0001, and with a weight decay parameter of
0.000001. In figure \ref{fig:mnist_samples}, samples from the components obtained
after training can be seen. Moreover, a normalized contingency table is presented,
where the performance of the variational posterior as a clustering function
can be assessed. Note that the cluster indices induced by the model have no
semantic meaning.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/trained_mnist.png}
  \caption{Samples from the fitted mixture components. Each row is sampled
  from the same component}
  \label{fig:mnist_samples}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\toprule
\diagbox[trim=lr]{True\\label}{Cluster\\index} &         0 &         1 &         2 &         3 &         4 \\
\midrule
0    &  0.000602 &  0.012432 &  0.002807 &  0.982555 &  0.001604 \\
1    &  0.002139 &  0.020146 &  0.977001 &  0.000178 &  0.000535 \\
2    &  0.000802 &  0.952276 &  0.011630 &  0.007219 &  0.028073 \\
3    &  0.001558 &  0.479455 &  0.300682 &  0.004284 &  0.214021 \\
4    &  0.646166 &  0.347273 &  0.005125 &  0.001435 &  0.000000 \\
\bottomrule
\end{tabular}
\caption{Normalized contingency table for the clustering induced by the model}
\label{table:contingency}
\end{table}

From table \ref{table:contingency} and figure \ref{fig:mnist_samples} it's possible
to see that although there is some confusion, the model successfully clusters
the MNIST digits.
