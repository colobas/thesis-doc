\section{Experiments}
\label{section:experiments}

In this section, the proposed model is applied to two benchmark synthetic datasets (Pinwheel
and Two-circles) and one real-world dataset (MNIST). On one of the synthetic
datasets, one shortcoming of the model is brought to attention, but is overcome
in a semi-supervised setting. On the real-world dataset, the model's clustering
capabilities are evaluted, as well as its capacity to model complex distributions.

A technique inspired in \autocite{mixae} was employed to improve training
speed and quality of results. This consists in dividing the inputs of the softmax layer in
the variational posterior by a \q{temperature} value, $T$, which follows
an exponential decay schedule during training. Intuitively, this makes the
variational posterior \q{more certain} as training proceeds, while allowing all
components to be generally exposed to the whole data, during the initial epochs.
This discourages components from being \q{subtrained} during the initial epochs
and, subsequently, from being prematurely discarded by the variational posterior.

\subsection{Toy datasets}
\subsubsection{Pinwheel dataset}

This dataset is constituted by five non-linear \q{wings}. See Figure \ref{fig:pinwheel}
for the results of running the model on this dataset. As expected, the variational
posterior has learned to partition the space so as to attribute each \q{wing} to
a component of the mixture. This partitioning is imperfect in regions of space
that have low probability for every component.

\begin{figure}
\centering
  \begin{subfigure}[t]{.5\textwidth}
    \raggedleft
    \includegraphics[width=0.6\linewidth]{figures/original_pinwheel.png}
  \end{subfigure}%
  \begin{subfigure}[t]{.5\textwidth}
    \raggedright
    \includegraphics[width=0.6\linewidth]{figures/trained_pinwheel.png}
  \end{subfigure}
  \caption{\scriptsize (a) Original dataset (b) Samples from the learned model. Each
dot is colored according to the component it was sampled from. The background
colors denote the regions where each component has maximum probability assigned
by the variational posterior. (Note that the background colors were chosen
so as to not match the dot colors, otherwise it dots wouldn't be visible)}
  \label{fig:pinwheel}
\end{figure}

\subsubsection{Two-circles dataset}
This dataset consists of two concentric circles. The experiment on this dataset,
shown on Figure \ref{fig:twocircles}, makes evident one shortcoming of this
model: the way in which the variational posterior partitions the space is
not necessarily guided by the intrisic structure in the data. In the case of
the two-circles dataset, it was found that the most common space partitioning
induced by the model consisted simply of splitting into two half-spaces. However, in
a semi-supervised setting, this behaviour can be corrected and the model
successfully learns to separate the two circles, as shown in Figure
\ref{fig:twocircles}-d). In this setting, the model was pretrained on
the labeled instances for some epochs and then trained with the normal procedure.
There were 1024 labeled instances, and 64 unlabeled instances.
In the semi-supervised setting, the model has the chance to refine both the
variational posterior and each of the components, thus making better use of
the unlabeled data in the unsupervised phase of the training. As is clearly
visible in Figure \ref{fig:twocircles}, the model struggles with
learning full, closed, circles; this is because it is unable to \q{pierce a hole}
in the base distribution, due to the nature of the transformations that are
applicable. Thus, to model a circle, the model has to learn to stretch the blob
formed by the base distribution, and \q{bend it over itself}. This difficulty
is also what keeps the model from learning a structurally interesting solution
in the fully unsupervised case: it is easier to learn to distort space so as to
learn a multimodal distribution that models half of the two circles. Moreover,
the points in diametrically opposed regions of the same circle are more dissimilar
(in the geometrical sense) than points in the same region of the two circles.
Therefore, when completely uninformed by labels, the variational posterior's
layers will tend to have similar activations for points in the latter case, and
thus tend to place them in the same class.

\begin{figure}
\centering
  \begin{subfigure}[t]{.24\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/original_2_circles.png}
  \end{subfigure}%
  \begin{subfigure}[t]{.24\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/trained_2_circles_2.png}
  \end{subfigure}
  \begin{subfigure}[t]{.24\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/labeled_2_circles.png}
  \end{subfigure}%
  \begin{subfigure}[t]{.24\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/trained_2_circles_semisup.png}
  \end{subfigure}
  \caption{\scriptsize a) Original dataset. b) Samples from the learned model, without any labels. Coloring logic
    is the same as in Figure \ref{fig:pinwheel}. c) Labeled points used in semi-supervised scenario.
    d) Samples from the model trained in the semi-supervised scenario.}
  \label{fig:twocircles}
\end{figure}

\subsection{Real-world dataset}
In this subsection, the proposed model is evaluated on the well-known MNIST
dataset \autocite{MNIST}. For this experiment, only the images corresponding to the digits
from 0 to 4 were considered. In Figure \ref{fig:mnist_samples}, samples from the
components obtained after training can be seen. Moreover, a normalized
contingency table is presented, where the performance of the variational posterior
as a clustering function can be assessed. Note that the cluster indices induced
by the model have no semantic meaning.
\begin{figure}
\begin{floatrow}
\floatbox{figure}[.49\textwidth][\FBheight][t]{%
  \includegraphics[width=0.90\linewidth]{figures/trained_mnist.png}%
}{%
  \caption{Samples from the fitted mixture components. Each row is sampled from the same component}%
  \label{fig:mnist_samples}%
}
\floatbox{table}[.49\textwidth][\FBheight][t]{%
\setlength\tabcolsep{1.5pt}
\begin{tabular}{c| *{5}{S[table-auto-round, table-format=-1.2]}}
 &         0 &         1 &         2 &         3 &         4 \\
\midrule
0    &  0.000602 &  0.012432 &  0.002807 &  0.982555 &  0.001604 \\
1    &  0.002139 &  0.020146 &  0.977001 &  0.000178 &  0.000535 \\
2    &  0.000802 &  0.952276 &  0.011630 &  0.007219 &  0.028073 \\
3    &  0.001558 &  0.479455 &  0.300682 &  0.004284 &  0.214021 \\
4    &  0.646166 &  0.347273 &  0.005125 &  0.001435 &  0.000000 \\
\bottomrule
\end{tabular}
}{%
\caption{Normalized contingency table for the clustering induced by the model.
Rounded to two decimal places. Rows: true label. Columns: cluster index.}%
\label{table:contingency}%
}
\end{floatrow}
\end{figure}%
%\begin{figure}[!htb]
%  \centering
%  \includegraphics[width=0.60\linewidth]{figures/trained_mnist.png}
%  \caption{Samples from the fitted mixture components. Each row is sampled
%  from the same component}
%  \label{fig:mnist_samples}
%\end{figure}
%
%\begin{table}[h]
%\centering
%\begin{tabular}{cccccc}
%\toprule
%\diagbox[trim=lr]{True\\label}{Cluster\\index} &         0 &         1 &         2 &         3 &         4 \\
%\midrule
%0    &  0.000602 &  0.012432 &  0.002807 &  \textbf{0.982555} &  0.001604 \\
%1    &  0.002139 &  0.020146 &  \textbf{0.977001} &  0.000178 &  0.000535 \\
%2    &  0.000802 &  \textbf{0.952276} &  0.011630 &  0.007219 &  0.028073 \\
%3    &  0.001558 &  \textbf{0.479455} &  0.300682 &  0.004284 &  0.214021 \\
%4    &  \textbf{0.646166} &  0.347273 &  0.005125 &  0.001435 &  0.000000 \\
%\bottomrule
%\end{tabular}
%\caption{Normalized contingency table for the clustering induced by the model}
%\label{table:contingency}
%\end{table}
From Table \ref{table:contingency} and Figure \ref{fig:mnist_samples} it is possible
to see that although there is some confusion, the model successfully clusters
the MNIST digits.
