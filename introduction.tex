\chapter{Introduction}
\label{chapter:introduction}

\section{Motivation and Related Work}
\label{section:motivation}

Generative models based on neural networks - variational autoencoders (VAEs),
generative adversarial networks (GANs), normalizing flows and their variations -
have experienced increased interest and progress in their capabilities. VAEs
\autocite{vaepaper} work by leveraging the reparameterization trick to optimize
a variational posterior parameterized by a neural network, jointly with the generative
model per se - it too a neural network, which takes samples from a latent distribution at its
input space and \emph{decodes} them into the observation space. GANs also work by
jointly optimizing two neural networks: a \emph{generator}, which learns to produce
realistic samples in order to \q{fool} the second network - the \emph{discriminator} - which
learns to distinguish samples produced by the generator from samples taken from real data. GANs learn
by having the generator and discriminator \q{compete}. Both VAEs and GANs learn
\emph{implicit} distributions of the data, in the sense that - if training is
successful - one can sample from the learned model, but there's no way to compute
the likelihood of the learned distribution. Normalizing flows differ from VAEs
and GANs in that they allow learning \emph{explicit} distributions of the
data\footnote{In fact, recent work \autocite{flowgan} combines the training
framework of GANs with the use of normalizing flows, so as to obtain a generator
for which it is possible to compute likelihoods.}. Thus, normalizing flows lend
themselves to the task of density estimation.

Less (although some) attention has been given to the extension of these types
of models with discrete structure, such as the one found in finite mixture models.
Exploiting such structure, while still being able to benefit from the expressiveness
of neural generative models - specifically, normalizing flows - is the goal of this
work. Concretely, this works explores a framework to learn a mixture of normalizing
flows. In practice, a neural network classifier is learned jointly with the mixture
components. Doing so will naturally produce an approach which lends itself not
only to density estimation, but also to clustering - since the classifier can be used
to assign points to clusters - and semi-supervised learning, where available
labels can be used to refine the classifier and selectively train the mixture
components.

The work presented here intersects several active directions of research.
In the sense of combining deep neural networks with probabilistic modelling,
particularly with the goal of endowing simple probabilistic graphical models
with more expressiveness, \textcite{svae} and \textcite{lin2018variational} propose a
framework to use neural-network-parameterized likelihoods, composed with latent
probabilistic graphical models. Still in line with this topic, but with an
approach more focused towards clustering and semi-supervised learning, \textcite{gmVAE}
proposes a VAE-inspired model, where the prior is a Gaussian mixture. \textcite{DEC}
describe an unsupervised method for clustering using deep neural networks, which
is a task that can also be fulfilled by the model presented in this work.

The two previous publications that are most related to the present work are
the following:

\textcite{RAD}, similarly to this work, try to reconcile normalizing flows with a
multimodal/discrete structure. They do so by partitioning the latent space into
disjoint subsets, using a mixture model where each component has non-zero
weight exclusively within its respective subset. Then, using a set identification
function and a piecewise invertible function, a variation of the change-of-variable
formula is devised.

\textcite{semisuplearning_nflows} also exploit multimodal structure while
using normalizing flows for expressiveness. However, while the present work relies on
a variational posterior parameterized by a neural network and learns $K$ flows
(one for each mixture component), the method proposed by \textcite{semisuplearning_nflows} resort to a
latent mixture of Gaussians as the base distribution for its flow model, and
learns a single flow.

\section{Objectives}
\label{section:objectives}

The objectives of the present work can be summarizd as follows:
\begin{itemize}
    \item designing a mixture of normalizing flows, with a tractable learning procedure;
    \item a proof-of-concept implementation to demonstrate the capabilities of such model, namely in the tasks of:
        \begin{itemize}
        \item Density estimation;
        \item Clustering;
        \item Semi-supervised learning.
        \end{itemize}
\end{itemize}

We have achieved these goals by proposing a method to learn a mixture of
$K$ normalizing flows, through the optimization of a variational inference
objective, where the variational posterior is parameterized by a neural network
with a softmax output, and its parameters are optimized jointly with those of
the mixture components.

\section{Thesis Outline}
\label{section:outline}

In Chapter 2, the concepts on probabilistic modelling needed for the remainder
of the work are introduced. In Chapter 3, the framework and theoretical background
of normalizing flows is presented. Chapter 4 describes in detail the model
proposed in the present work, and Chapter 5 contains empirical results and their
interpretation.

\section{Notation}
\label{section:notation}
The main notation used throughout this work is as follows:

\begin{itemize}
    \item Scalars and vectors are lower-case letters. To differentiate between
them, vectors will be present in bold. E.g.: $x$ is a scalar $\mathbf{z}$ is
a vector.
    \item Upper-case letters are matrices.
    \item For vectors, $\bm{x}_{a:b}$ denotes the $a$-th to the $b$-th elements
    of $\bm{x}$.
    \item For distributions, subscript notation will only be used when the
distribution isn't clear from context.
    \item The operator $\odot$ denotes the element-wise product.
    \item The letter $x$ is preferred for observations.
    \item The letter $z$ is preferred for latent variables.
    \item The letter $\bm\theta$ is preferred for parameter vectors.
    \item A function $g$ of $\bm{x} \in \bm{\mathcal{X}}$, which is parameterized
    by a parameter vector $\bm\theta$ is written as $g(\bm{x};\bm\theta)$, when
    the dependence on $\bm\theta$ is to be made explicit.
\end{itemize}
