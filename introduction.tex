\section{Introduction}
\label{section:introduction}

Generative models based on neural networks - variational autoencoders (VAEs),
generative adversarial networks (GANs), normalizing flows and their variations -
have experienced increased interest and progress in their capabilities. Both VAEs and GANs learn
\emph{implicit} distributions of the data, in the sense that - if training is
successful - one can sample from the learned model, but the likelihood function
of the learned distribution is not accessible. Normalizing flows differ from VAEs
and GANs in that they allow learning \emph{explicit} distributions of the
data\footnote{In fact, recent work \autocite{flowgan} combines the training
framework of GANs with the use of normalizing flows, so as to obtain a generator
for which it is possible to compute likelihoods.}. Thus, normalizing flows lend
themselves to the task of density estimation.

The goal of this work is to employ normalizing flows in a finite mixture model, 
so as to better model multimodal datasets. In practice, a neural network classifier
is learned jointly with the mixture components. Doing so will naturally produce
an approach which lends itself not only to density estimation, but also to
clustering - since the classifier can be used to assign points to clusters - and
semi-supervised learning, where available labels can be used to refine the classifier
and selectively train the mixture components.

\textcite{RAD}, similarly to this work, try to reconcile normalizing flows with a
multimodal/discrete structure, by partitioning the latent space into disjoint
subsets, using a mixture model where each component has non-zero weight
exclusively within its respective subset. Then, using a set identification
function and a piecewise invertible function, a variation of the change-of-variable
formula is devised.
\textcite{semisuplearning_nflows} also exploit multimodal structure while
using normalizing flows for expressiveness. However, while the present work relies on
a variational posterior parameterized by a neural network and learns $K$ flows
(one for each mixture component), \textcite{semisuplearning_nflows} resort to a
latent mixture of Gaussians as the base distribution for its flow model, and
learn a single flow.
%\subsection{Notation}
%\label{section:notation}
%Scalars and vectors are lower-case letters, with vectors in bold. Upper-case letters
%represent matrices. Vector $\bm{x}_{a:b}$ denotes the $a$-th to the $b$-th elements
%of vector $\bm{x}$. For distributions, subscript notation will only be used when the
%distribution isn't clear from context. The operator $\odot$ denotes the element-wise product.
%A function $g$ of $\bm{x} \in \bm{\mathcal{X}}$, parameterized by a vector $\bm\theta$,
% is written as $g(\bm{x};\bm\theta)$, when the dependence on $\bm\theta$ is to be made explicit.
