\chapter{Normalizing Flows}
\label{chapter:probmodel}

\section{Introduction}
The best known and studied probability distributions are rarely expressive
enough for real-world datasets. However, they have properties that make them
amenable to work with, for instance: tractable parameter estimation and closed-form
likelihood functions.

As has been described, one way to obtain more expressive models is to assume the
existence of latent variables, leverage certain factorization structures, and to
use well-known distributions for the individual factors of the product that
constitutes the model's joint distribution. By using these structures and choosing
specific combinations of distributions (namely, conjugate prior-likelihood pairs),
these models are able to stay tractable - normally via bespoke estimation/inference/learning
algorithms.

%Structures are easily encoded by Probabilistic Graphical Models, which are a
%framework to easily express conditional independence assumptions and to specify
%complex probabilistic models.

Another approach to obtaining expressive probabilistic models is to apply
transformations to a simple distribution, and use the Change of Variables
formula to compute probabilities in the transformed space. This is the basis
of Normalizing Flows, an approach proposed by \author{shakir_nf} in \cite{shakir_nf},
and which has since evolved and developed into the basis of multiple SoA techniques
for density estimation [TODO: cite examples here].

\section{Change of Variables}
Given a probability distribution $p(z)$, with probability density function $f_Z(.)$,
and a bijective and continuous function $g$, it's possible to write an expression
for the probability density function $f_Y(.)$ of the random variable $y$ that is
obtained by applying $g$ to samples of $p(z)$:
\begin{align}
    \mbox{if } z &\sim p(z) \\
    \mbox{and } y &= g(z) \\
    \mbox{then } f_Y(y) &= f_Z(g^{-1}(y))\Big|\det\Big(\frac{d}{dy}g^{-1}(y)\Big)\Big|
\end{align}

For this to be useful, some objects have to be easily computable:
\begin{itemize}
    \item $f_Z(z)$ - the starting distribution's probability density function.
        It is assumed that there is a closed-form expression to compute this. In
        practice, this is normally one of the basic distributions (Gaussian,
        Uniform, etc.)
    \item $\det\Big(\frac{d}{dy}g^{-1}(y)\Big)$ - the determinant  of the Jacobian
        matrix of $g^{-1}(.)$ . For most transformations this is not "cheap" to compute.
        As will be shown, the main challenge of Normalizing Flows is to find
        transformations that are expressive and for which 
        the determinants of their Jacobian matrices are "cheap" to compute.
\end{itemize}

\section{Normalizing Flows}
Let us have $L$ transformations $h_\ell$ that fulfill the above two points, and
let $z_\ell$ be the result of applying transformation $h_{\ell-1}$, with the
exception of $z_0$, which is obtained by sampling from $p(z_0)$, the base distribution.
Furthermore, let $g$ be the composition of the $L$ transformations.

Applying the Change of Variables formula:
\begin{align}
    \mbox{if } z_0 &\sim p(z_0) \\
    \mbox{and } y &= h_{L-1} \circ h_{L-2} \circ ... \circ h_0(z_0) \\
    \mbox{then } f_Y(y) &= f_Z(g^{-1}(y))\Big|\det\Big(\frac{d}{dy}g^{-1}(y)\Big)\Big| \\
                        &= f_Z(g^{-1}(y))\prod_{\ell=0}^{L-1}\Big|\det\Big(\frac{d}{dz_{\ell+1}}h_{\ell}^{-1}(z_{\ell+1})\Big)\Big| \\
                        &= f_Z(g^{-1}(y))\prod_{\ell=0}^{L-1}\Big|\det\Big(\frac{d}{dz_{\ell}}h_{\ell}\Big(h_{\ell}^{-1}(z_{\ell+1}\Big)\Big)\Big|^{-1} \label{eq:nflowderivation}
\end{align}

Replacing $h_{\ell}^{-1}(z_{\ell+1}) = z_\ell$ in \ref{eq:nflowderivation}:

\begin{align}
         f_Y(y) &= f_Z(g^{-1}(y))\prod_{\ell=0}^{L-1}\Big|\det\Big(\frac{d}{dz_{\ell}}h_{\ell}\Big(z_\ell)\Big)\Big)\Big|^{-1} \\
    \log f_Y(y) &= \log f_Z(g^{-1}(y)) - \sum_{\ell=0}^{L-1} \log \Big|\det\Big(\frac{d}{dz_{\ell}}h_{\ell}(z_\ell)\Big) \Big| \label{eq:nflowsfinal}
\end{align}

Depending on the task, one might prefer to replace the second term in \ref{eq:nflowsfinal}
with a sum of log-abs-determinants of the Jacobians of the inverse transformations.
This switch would imply replacing the minus sign before the sum with a plus sign:
\begin{align}
    \log f_Y(y) &= \log f_Z(g^{-1}(y)) + \sum_{\ell=0}^{L-1} \log \Big|\det\Big(\frac{d}{dz_{\ell+1}}h_{\ell}^{-1}(z_{\ell+1})\Big) \Big|
\end{align}

With this expression, gradient-based MLE becomes feasible. Moreover, sampling
from the resulting distribution is simply achieved by sampling from the base
distribution and applying the chain of transformations. Because of this, Normalizing
Flows lend themselves to be used as flexible variational posteriors, in Variational
Inference settings.

\section{Examples of transformations}
\subsection{Affine Transformation}
The simplest transform that can be applied is an Affine Transformation.
This transformation can stretch, shrink and rotate space. It is simply the 
multiplication by a matrix $A$ and summation of a bias vector $b$:
\begin{align}
    z &\sim p(z) \\
    y &= Az + b
\end{align}

The Jacobian of this transformation is simply the determinant of the matrix $A$.

However, in general, computing the determinant of a $N \times N$ matrix has a
complexity of $\mathcal{O}(N^3)$. For that reason, it is common to use matrices
with a certain structure that makes their determinants easier to compute. For
instance, if $A$ is triangular, its determinant is simply the product of its
diagonal's elements. The downside of using matrices that are constrained to
a certain structure is that they correspond to less flexible transformations.

It is possible, however, to design Affine Transformations whose Jacobians are
of $\mathcal{O}(N)$ complexity and that are more expressive than simple
triangular matrices. In \cite{Glow}, one such transformation is proposed. It
constrains the matrix $A$ to be decomposable as $A = PL\big(U + diag(\mathbf{s})\big)$,
where $diag(\mathbf{s})$ is a diagonal matrix whose diagonal's elements are
the values of vector $\mathbf{s}$. The following additional constrains are in place:
\begin{itemize}
    \item $P$ is a permutation matrix
    \item $L$ is a lower triangular matrix, with ones on its diagonal
    \item $U$ is an upper triangular matrix, with zeros on its diagonal
\end{itemize}

Given these constraints, the determinant of the matrix $A$ is simply the product
of the elements of $\mathbf{s}$.

\subsection{PReLU Transformation}
\subsection{Affine Coupling Transformation}
\subsection{Batch-Norm Transformation}

