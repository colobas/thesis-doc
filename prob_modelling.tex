\chapter{Probabilistic Modelling}
\label{chapter:probmodel}

\section{Introduction}
\label{section:probmodelintro}
The goal of probabilistic modelling is a to leverage probability distributions
and random variables to posit, test, and refine hypothesis about the behaviour of
systems that include stochastic components. Given observations of a system, the
task of probabilistic modelling normally boils down to finding a probability
distribution which:
\begin{itemize}
    \item is consistent with the observed data;
    \item is consistent with \textbf{new}, previously unobserved data, originated\
        in the same system.
\end{itemize}

This probability distribution is commonly called the \emph{model}. A good model
should be a good \emph{emulator} of the true underlying generative process that originated
the observed data. In informal terms, this can be summarized as:
\begin{align}
    \mbox{data} \sim p(\mbox{data}|\mbox{hypothesis}^*),
\end{align} where $\text{hypothesis}^*$ is the optimal hypothesis. Via Bayes' Law,
we can write, still informally,
\begin{align}
    p(\mbox{hypothesis}|\mbox{data}) = \frac{p(\mbox{data}|\mbox{hypothesis})p(\mbox{hypothesis})}{p(\mbox{data})} \label{eq:bayes}.
\end{align}

In practice, a modeller will search for an hypothesis that maximizes (some form,
or approximation of) this expression. For simpler problems, this search happens
analytically, hopefully in closed-form, i.e., there is an expression to compute the optimal hypothesis,
given observed data. However, for most real-world problems, there is no closed-form solution,
and the modeller has to resort to algorithms and approximations, and will only
be able to find \textbf{local} optima for the above expression, in most cases.

It is also worth noting that there are effectively infinite candidate distributions -
each one an hypothesis for how the system at hand generates data. It is common
to make use of domain knowledge and assume the true system has a certain intrinsic
structure and form, and to use these assumptions to constrain the space of
candidate hypothesis. Assumptions about structure usually translate to conditional
independence claims between some or all of the observed variables; assumptions
about form translate into the use of specific parametric families to govern some
or all of the observed variables. These assumptions are commonly connected between
themselves (for example, when conjugate likelihood-prior pairs are used). When parametric
forms are used, an hypothesis is uniquely defined by the set of parameters it
requires - commonly denoted $\bm{\theta}$. For the type of models and problems
dealt with in this work, we will assume $\bm{\theta}$ is finite dimensional, but it is worth
noting that there are models for which the dimension of $\bm{\theta}$ \emph{grows}
with the dataset size. These are called non-parametric models. They come with
their own advantages and disadvantages, which are out of the scope of this work..

\section{Model Complexity}
\label{section:modelcomplexity}
Intuitively, the dimension of $\bm{\theta}$ is deeply connected with the expressiveness
of the distribution. In practice, this translates to the observation that if we
make the model\footnote{Throughout this work, the words \emph{model}
and \emph{distribution} will be used almost interchangeably, making it clear when context isn't
enough.} expressive enough, it can fit the observed data arbitrarily well. Naïvely,
this would be a desirable characteristic to exploit - it is always possible to increase
the likelihood by adding parameters to the model. However, increasing model
complexity normally comes at the expense of generalization. This phenomenon is
commonly referred to as \emph{overfitting}, and there are several
angles from which to explain it and interpret it. Namely:
\begin{itemize}
    \item The classical perspective is that of the \emph{bias-variance tradeoff}. To
        understand this, consider the concept of an hypothesis class - a set
        of hypotheses in which, via some procedure, the modeller will search
        for an hypothesis that is consistent with the observed data, and is
        expected to generalize to unseen data. Said procedure is what is normally
        referred to as \emph{fitting} the model to the data. In the case of
        parametric models, the set of models of a given parametric form, with
        a parameter-vector of a certain fixed dimension, is an example of an hypothesis
        class. Intuitively, a more complex hypothesis class is more likely to
        contain the true hypothesis (or a good approximation to it). However,
        the more complex the hypothesis class, the larger the search space -
        the higher the number of candidate hypothesis. In this sense, an increase
        in the size of the search space often translates into an increase of the
        sensitivity to the problem variables (in the case of learning and inference,
        this means sensitivity to initialization and to the data used to fit).
        Conversely, a simpler model will constitute a smaller search-space, hence
        the search procedure will be less sensitive to initalization and problem
        variables. However, the true hypothesis (or a good approximation to it)
        is less likely to be contained in it - precisely because it is a smaller
        hypothesis class. The bias-variance tradeoff is a summary of these observations:
        a highly complex model is potentially able to achieve a low expected error
        on observed data (low bias), but will tend to be extremely sensible to small
        variations on its input (high variance). Conversely, a simpler model will
        be more robust to variations on its input (low variance), but won't have the same
        modelling capacity and will produce a larger expected error (high bias).
        The number of parameters is far from being the best measure of complexity
        of a model. Nevertheless, it is a good proxy to compare model complexity
        between models of the same parametric family. However, recent work by
        \textcite{Belkin2018Dec} shows that modern machine learning
        contexts, in which the number of parameters is far larger than in classical
        settings, have to be understood under a measure of model complexity
        different from the traditional ones. This is because it is now common
        practice to fit highly overparameterized models to a point of interpolation
        (close to zero training error), still being able to achieve good generalization -
        which evidently contradicts the classical view.
    \item Andrey Kolmogorov's and Gregory Chaitin's ideas on algorithmic information
        theory (AIT) \autocite{chaitin-leibniz}, and the so-called Kolmogorov complexity  are another
        useful lens through which to regard this question. Consider that data are
        measurements of phenomena. Modelling is concerned with finding the
        laws that explain/govern these phenomena. Intuitively, if the laws are
        as complex as the data they intend to explain, they aren't explaining anything.
        AIT formalizes this notion by borrowing the concept of \emph{program} to
        define the generative process by which observed data comes to existence.
        The complexity of a dataset is then easy to define: it is the size of
        the \textbf{smallest}\footnote{Note the emphasis on \q{smallest} - this is
        because any program can be made arbitrarily redundant, and thus arbitrarily large.}
        program that generates the observed data. The appropriate unit with
        which to measure the size of a program - and, as we have now seen, the
        complexity of a dataset - is bits\footnote{Or the basic unit of memory
        of the computer where the data generating program would run}. The parallel
        between these ideas and the question of overfitting is thus easy to make:
        a program (or a model and its parameter vector)  is useful if it 
        \emph{compresses} the data, intuitively because to do so it leverages the
        patterns therein, which are the object of interest in the modelling task.
\end{itemize}

Both of these lines of reasoning make clear that there is a certain balance
in complexity that a good model has to achieve: it should be parsimonious enough
that it won't overfit, but flexible enough that it is able to properly explain
the observed data.  There are strategies to make this mathematically objective.
Some of those methods are the Bayesian Information Criterion \autocite{bic}, the Akaike
Information Criterion \autocite{aic} and the Minimum Description Length \autocite{Lanterman_2001}.

\section{MAP and ML Estimation}
\label{section:map-mle}

Once the parametric form of the model is defined, the task at hand becomes the
discovery of the parameter vector $\bm{\theta}$ that best explains the observed
data, $\bm{x}$, within the defined parametric family. The naïve (but often the only
possible) approach is to maximize what is called the likelihood function. That
is, given observation $x$, the maximum likelihood (ML) estimate of $\bm\theta$ is
given by:
\begin{align}
    \bm{\hat\theta}_{ML} = \argmax_{\bm{\theta}} \mathcal{L}(\bm{\theta}),
\end{align} where $\mathcal{L}(\bm{\theta}) = p(x | \bm{\theta})$.
Depending on the model, finding $\bm{\hat\theta}_{ML}$ - the optimum -
can be as simple as computing an analytical expression, or as difficult as using
gradient-based methods to optimize a non-convex objective. In the latter case,
local optima are usually the best one can expect to obtain.

Another approach, called \emph{maximum a posteriori}, works by retrieving the
mode (or one mode, if there are several) of the posterior distribution of the
parameter-vector, given by:
\begin{align}
    \bm{\hat\theta}_{MAP} = \argmax_{\bm{\theta}} p(\bm{\theta} | x),
\end{align} where,
\begin{align}
    p(\bm{\theta} | x) = \frac{p(x |\bm{\theta})p(\bm{\theta})}{p(x)}.
\end{align}
It is easy to see that these two approaches are intimately related. MAP differs
from ML by the fact that it employs the prior $p(\bm{\theta})$ to give different
weights to different hypothesis (i.e., different instances of $\bm{\theta}$). This
is useful if there is domain knowledge available that can be encoded in the
prior. It is clear that ML is a special case of MAP, when the prior is
uniform.

\section{Structure and Latent Variables}
\label{section:probmodellatvar}
In some cases, one might want to leverage some available domain knowledge. This
often translates into assuming that there is some latent structure in the data.
This structure is commonly encoded into latent variables and their influence over
the observable variables. In this scenario, we become interested in the distribution
given by
$p(\bm{x}, \bm{z}, \bm{\theta_x}, \bm{\theta_z})$, where $z$ is the latent variable, $\bm{\theta_x}$ is
the parameter vector for the distribution of $\bm{x} \in \bm{\mathcal{X}}$, and $\bm{\theta_z}$ is
the parameter vector for the distribution of $\bm{z} \in \bm{\mathcal{Z}}$.

For structure and latent variables to be useful, it is common to make the additional
assumption that we have the ability of factorizing their joint distribution in ways that
make it tractable. If we have a dataset $\bm{X}$, with $N$ i.i.d. samples $\bm{x_1}, \bm{x_2}, \bm{x_3}, ..., \bm{x_N}$
and $N$ latent variables $\bm{z_1}, \bm{z_2}, \bm{z_3}, ..., \bm{z_N}$, one common factorization is:
\begin{align}
    p(\bm{X}, \bm{Z}, \bm{\theta}) = \Big(\prod^N_{i=1} p(\bm{x_i}| \bm{z_i}, \bm{\theta_x})
                                                        p(\bm{z_i} | \bm{\theta_z})\Big) 
                                                        p(\bm{\theta_x})
                                                        p(\bm{\theta_z}). \label{eq:latentvariable}
\end{align}

It is also possible that the samples in $\bm{X}$ have some sort of causal
relation, for instance if they occur ordered in time. In this case, they are not i.i.d.
One way to encode this assumption is to posit an \emph{autoregressive} model, i.e.,
a model in which a random variable depends on the realizations of the variables
that come before it. If each random variable conditionally depends solely on the
random variable that precedes it, this is called a (first-order) Markov model.
A common variation of the Markov model is the hidden Markov Model, where the
autoregressive part of the model is present only in the latent variables:
\begin{align}
    p(\bm{X}, \bm{Z}, \bm{\theta}) = p(\bm{z_1}) \Big(\prod^N_{i=2} p(\bm{x_i} | \bm{z_i}, \bm{\theta_x})
                                     p(\bm{z_i}| \bm{z_{i-1}}, \bm{\theta_z}) \Big)
                                     p(\bm{\theta_x}) p(\bm{\theta_z}).
\end{align}

These are merely (classical) examples of models with different structure assumptions encoded
into them. Normally, if the structure has a certain regularity, it is possible
to exploit it to obtain tractable (approximate) inference and estimation methods.
This notion is exploited in the following section, for a subset of the family
of models described by (\ref{eq:latentvariable}.)

\section{Mixture Models and the EM Algorithm}
\label{section:mmodels}
Mixture models are a subset of the structure \q{family} described in (\ref{eq:latentvariable}),
and they have a central role in this work. In a mixture model there is a discrete
(unidimensional) latent variable $z_i$ which selects one of $K$ components from which an observation
$\bm{x_i}$ will be sampled. This can be summarized as
\begin{align}
    z_i \sim p(z_i | \bm\pi) \\
    x_i \sim p(\bm{x_i} | z_i),
\end{align} where $\pi_k$ is the the probability of $\bm{x_i}$ being sampled
from component $k$ (that is, the proability of $z_i = k$). This is commonly referred
to as the \emph{weight} of component $k$. $\bm\pi$ is the vector that contains
the weights of all components.
It is common to assume that the $K$ components are part of the same
parametric family. In that case, we can rewrite the above as:
\begin{align}
    z_i \sim p(z_i | \bm\pi) \\
    x_i \sim p(\bm{x_i} | \bm{\theta}_{z_i}),
\end{align} where it is made evident that the discrete variable $z_i$ is selecting
the \textbf{parameter vector} to be used for sample $\bm{x_i}$. By far the most
discussed (and used) mixture model is the Gaussian mixture model, in which the
$K$ components of the model are all Gaussian distributions with different means
and/or covariances.

\subsubsection{The EM Algorithm}
Although it is of much more general applicability, the expectation-maximization
(EM) algorithm is the most commonly used algorithm to fit a mixture model, i.e.
to estimate its parameters. The starting point of EM is the realisation that if
all variables were observed, it would be easy to apply ML or MAP estimation for
the parameters. Given that, the algorithm can be generally described as an alternation
between two steps:
\begin{itemize}
    \item \textbf{E-step}: infer the unobserved variables.
        (In the case of mixture models, this corresponds to inferring the discrete
        variables that select the component from which each observed data point
        was sampled. This can be roughly thought of as a cluster assignment).
    \item \textbf{M-step}: given the observed variables and the values inferred
        on the previous step, optimize the model parameters. (In the case of 
        mixture models, this correponds to inferring the parameters of each
        component, and its weight).
\end{itemize}

A more rigorous description of the procedure follows. Consider the expression
\begin{align}
    \ell_c(\bm{\theta}) = \sum^N_{i=1} \log p(\bm{x_i}, z_i | \bm{\theta}),
\end{align} which is the so-called complete-data log-likelihood. Like the likelihood
function, it is a function of $\bm{\theta}$, which is easy to compute, given all
the $\bm{x_i}$ and $z_i$.
However, the $z_i$ aren't observed. To overcome this, let us define the expected
complete-data log likelihood:
\begin{align}
    Q(\bm{\theta}, \bm{\theta}^{(t-1)}) = \mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\ell_c(\bm{\theta}) | X, \bm{\theta}^{(t-1)}],
\end{align} where $\bm{\theta}^{(t)}$ represents the value of $\bm{\theta}$ at time step
$t$ of the procedure. Note that the expectation is w.r.t. $Z$ (which is not observed),
given $X$ (which is observed)
and $\bm{\theta}^{(t-1)}$; it is the expected value of $\ell_c(\bm{\theta})$, given
the parameter values obtained at the previous step of the the algorithm. Depending
on the nature of $Z$, this expectation can be obtained either in closed form or
approximated, for instance, via samples of $z_i$ (if a sampling procedure is available).

In the case of mixture models, where the $z_i$ are instances of a categorical
random variable, the expression for $Q(\bm{\theta}, \bm{\theta}^{(t-1)})$ can be made
simpler as such:
\begin{align}
    Q(\bm{\theta}, \bm{\theta}^{(t-1)}) &= \mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\sum_{i=1}^N \log p(x_i, z_i | \bm{\theta})] \\
    &= \mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\sum_{i=1}^N \log \prod_{k=1}^K \big(\pi_k p(x_i | \bm{\theta}_k)\big)^{\mathbb{I}(z_i = k)}] \\
    &= \mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\sum_{i=1}^N \sum_{k=1}^K \log\Big(\big(\pi_k p(x_i | \bm{\theta}_k)\big)^{\mathbb{I}(z_i = k)}\Big)] \\
    &= \mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\sum_{i=1}^N \sum_{k=1}^K \mathbb{I}(z_i = k) \log\Big(\pi_k p(x_i | \bm{\theta}_k)\Big)] \\
    &= \sum_{i=1}^N \sum_{k=1}^K \underbrace{\mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\mathbb{I}(z_i = k)]}_{\equiv r_{ik}} \log\Big(\pi_k p(x_i | \bm{\theta}_k)\Big)\\
    &= \sum_{i=1}^N \sum_{k=1}^K r_{ik} \log\pi_k + \sum_{i=1}^N \sum_{k=1}^K r_{ik} \log p(x_i | \bm{\theta}_k) \label{eq:qthetatheta}
\end{align}

The quantity defined above as $r_{ik}$ is often referred to as the \emph{responsability}. It
is trivial to arrive at a closed-form expression for it, given the value of $\bm{\theta}$
arrived at on the previous iteration, i.e., $\bm{\theta}^{(t-1)}$,
\begin{align}
r_{ik} &= \mathbb{E}_{Z|X, \bm{\theta}^{(t-1)}}[\mathbb{I}(z_i = k)] \\
    &= p(z_i = k | x_i\ ; \bm{\theta}^{(t-1)}) \\
    &= \frac{p(z_i = k , x_i\ ; \bm{\theta}^{(t-1)})}{p(x_i\ ; \bm{\theta}^{(t-1)})} \\
    &= \frac{p(z_i = k , x_i\ ; \bm{\theta}^{(t-1)})}{\sum_{k'} p(z_i = k' , x_i\ ; \bm{\theta}^{(t-1)})} \\
    &= \frac{\pi_k p(x_i | \bm{\theta}_k^{(t-1)})}{\sum_{k'} \pi_{k'} p(x_i | \bm{\theta}_{k'}^{(t-1)})}
\end{align}

Since $r_{ij}$ is the posterior probability of observation $x_i$ having been
generated by component $j$, we have $\sum_j r_{ij} = 1$.

Finally, the EM algorithm for mixture models alternates between the following two steps:
\begin{itemize}
    \item \textbf{E-step}: Compute the responsabilies $r_{ik}$ for each $x_i$.
    \item \textbf{M-step}: Given $r_{ik}$, solve the following optimization
        problem:
        \begin{align}
            \bm{\theta^{(t)}} = \argmax_{\bm{\theta}} Q(\bm{\theta}, \bm{\theta^{(t-1)}})
                \underbrace{+ \log p(\bm{\theta})}_\text{Optional, in the case of MAP estimation},
        \end{align} where $Q(\bm{\theta}, \bm{\theta^{(t-1)}}$ is as given in \ref{eq:qthetatheta}
        This maximization may have a closed-form solution in some of the simplest
        mixture models, like Gaussian mixture models, but it may require a gradient-based
        optimization procedure for more flexible models.
\end{itemize}

\section{Approximate Inference}
\label{section:probmodelinf}
Consider the joint probability function of the observed data, latent variables,
and parameters $p(\bm{x}, \bm{z}, \bm{\theta})$. For simplicity, let redefine the
latent variables $\bm{z}$ to also include $\theta$. This means that the model is simply written
as the joint distribution $p(\bm{x}, \bm{z})$. To perform inference\footnote{If we
hadn't collapsed $\bm{\theta}$ into $\bm{z}$ and were instead handling it separatly,
we would call \textbf{inference} to the task of finding $\bm{z}$ and \textbf{learning}
to the task of finding $\bm{\theta}$} about $\bm{z}$, conditioned on $\bm{x}$, it
is necessary to find the posterior distribution of $\bm{z}$, given $\bm{x}$, i.e.,
$p(\bm{z}|\bm{x})$. Using Bayes' law:

\begin{align}
    p(\bm{z}|\bm{x}) &= \frac{p(\bm{x}|\bm{z})p(\bm{z})}{p(\bm{x})} \\
                     &= \frac{p(\bm{x}|\bm{z})p(\bm{z})}{\int p(\bm{x}|\bm{z'})p(\bm{z'}) d\bm{z'}}.
\end{align}

For the vast majority of cases, the integral in the denominator will be
intractable. To overcome this difficulty, it is common to resort to two families
of methods: Monte-Carlo methods (which are based on approximating this integral
using samples) - which are out of the scope of this work - and variational methods,
which are a central tool in this work.
%
%\subsection{Monte-Carlo Methods}
%\label{subsection:mcmc}
%
%Monte-Carlo methods work by using sampling techniques to approximate the
%intractable integral. The most powerful subclass of these methods is called
%Markov-chain Monte-Carlo. Its approach consists of devising a scheme that
%allows for sampling from a distribution close to the one of interest. It
%accomplishes this by defining a Markov-Chain whose transition function 
%is guaranteed to make it converge asymptotically to the distribution of interest,
%given some constraints (ergodicity...) (TODO: explain more)

\subsection{Variational Methods}
\label{subsection:variational}
Variational methods work (essentially) by turning the problem of integration into one of
optimization. These methods adopt a family of parametric distributions, and then
optimize the corresponding parameters so as to minimize the \q{distance} between the approximate
(normally called \q{variational}) distribution and the distribution of interest.
There are two ways to derive the most commonly used objective function for
this problem, which will be detailed in the two following subsections.

\subsubsection{Kullback-Leibler Divergence}
\label{subsubsection:kldiv}

The Kullback-Leibler divergence is a measure of the dissimilarity between two probability
distributions $p$, and $q$, given by, in the case of continuous variables,
\begin{align}
    KL(q||p) = \int q(z) \log\frac{q(z)}{p(z)} dz .
\end{align} For discrete variables, the integral is replaced by a sum.

In the inference setting, $p$ is the posterior $p(\bm{z}|\bm{x})$ and $q$ is a distribution
in some parametric family, with parameters $\phi$, i.e., $q(\bm{z}; \phi)$. However,
it is clear that we can't compute the Kullback-Leibler directly, because it
would require the knowledge of both distributions, and finding $p(\bm{z}|\bm{x})$ is
precisely the task at hand. Let us expand the KL divergence expression:
\begin{align}
    KL(q||p) &= \int q(\bm{z}) (\log q(\bm{z}) - \log p(\bm{z}|\bm{x})) dz \\
             &= \int q(\bm{z}) (\log q(\bm{z}) - (\log p(\bm{x}, \bm{z}) - \log p(\bm{x}))) d\bm{z} \\
             &= \mathbb{E}_q [\log q(\bm{z})] - \mathbb{E}_q [\log p(\bm{x}, \bm{z})] + \log p(\bm{x})
\end{align}
Consequently, minimizing this KL divergence is equivalent to maximizing
\begin{align}
    \mathbb{E}_q [\log p(\bm{x}, \bm{z})] - \mathbb{E}_q [\log q(\bm{z})]. \label{eq:elbokldiv}
\end{align} This quantity is commonly refered to as ELBO - Evidence Lower BOund \autocite{Blei2016Jan}.
It can be rewritten as:
\begin{align}
    ELBO(q) &= \mathbb{E}_q [\log p(\bm{x}, \bm{z})] - \mathbb{E}_q [\log q(\bm{z})] \\
            &= \mathbb{E}_q [\log p(\bm{x}|\bm{z})] + \mathbb{E}_q [\log p(\bm{z})] - \mathbb{E}_q [\log q(\bm{z})]
\end{align}
In this form, each term of the ELBO has an easily interpretable role:
\begin{itemize}
    \item $\mathbb{E}_q [\log p(\bm{x}|\bm{z})]$ tries to maximize the conditional likelihood of $\bm{x}$; it
        can be seen as assigning high probability mass to values of $\bm{z}$ that \emph{explain} $\bm{x}$
        well.
    \item $\mathbb{E}_q [\log p(\bm{z})]$ is the symmetric of the crossentropy between
        $q(\bm{z})$ and $p(\bm{z})$; maximizing this quantity is equivalent of minimizing
        that crossentropy. This can be regarded as a regularizer that discourages
        $q(\bm{z})$ from being too different from the prior $p(\bm{z})$.
    \item $ - \mathbb{E}_q [\log q(\bm{z})]$ is the entropy of $q(\bm{z})$; maximizing
        this term incentivizes the probability mass of $q(\bm{z})$ to be spread out:
        another form of regularization.
\end{itemize}

\subsubsection{Lower Bound on $\log p(\bm{x})$}
\label{subsubsection:elbo}

Another way of approaching the intractable posterior is to start by stating
that our inherent goal is to maximize $p(\bm{x})$, or equivalently $\log p(\bm{x})$. Given
that, consider the following chain of equalities and inequalities,
\begin{align}
    \log p(\bm{x}) &= \log \int p(\bm{x}, \bm{z}) d\bm{z}\\
    &= \log \int q(\bm{z}) \frac{p(\bm{x}, \bm{z})}{q(\bm{z})} d\bm{z} \\
    &= \log \mathbb{E}_q[\frac{p(\bm{x}, \bm{z})}{q(\bm{z})}] \label{eq:elbojensen1} \\
    &\geq \mathbb{E}_q[\log \frac{p(\bm{x}, \bm{z})}{q(\bm{z})}] \label{eq:elbojensen2} \\
    &\geq \mathbb{E}_q[\log p(\bm{x}, \bm{z})] - \mathbb{E}_q[\log q(\bm{z})] \label{eq:elbojensen3},
\end{align}
where the inequality between (\ref{eq:elbojensen1}) and (\ref{eq:elbojensen2})
results from Jensen's inequality, given (in one of its forms) by:
\begin{align}
    \phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)], \label{eq:jensen}
\end{align} where $\phi$ is a convex function. Obviously, the inequality is
reversed for a concave function, as is the case of the logarithm.

Note that the right-hand side of (\ref{eq:elbojensen3}) is the same quantity
we arrived at in (\ref{eq:elbokldiv}), and that it is a lower-bound on the
quantity we want to maximize, so it should also be maximized. It is worth noting
that when $q(\bm{z}) = p(\bm{z}|\bm{x})$, the bound is tight.
