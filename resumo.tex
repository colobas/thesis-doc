\section*{Resumo}

% Add entry in the table of contents as section
\addcontentsline{toc}{section}{Resumo}

Nos últimos anos, os modelos generativos profundos, tais como redes generativas
adversariais (\autocite{GAN}) e auto-codificadores variacionais \autocite{vaepaper},
e as suas variantes, têm vindo a ser amplamente adoptados para a tarefa de modelação
de distribuições de probabilidade a partir de dados. Apesar da qualidade excepcional das amostras sintéticas
que estes modelos são capazes de produzir, as distribuições das mesmas são aprendidas
\emph{implicitamente}, no sentido em que não é possível aceder explicitamente às funções de densidade
de probabilidade por eles induzidas. Esta característica torna-os inadequados para tarefas que
requeiram, por exemplo, avaliar novas amostras de dados com as distribuições
aprendidas. Os \emph{fluxos normalizantes} ultrapassam esta limitação tirando partido da
fórmula de mudança de variável para distribuições de probabilidade e da utilização
de transformações desenhadas para ter jacobianas tratáveis e computáveis de forma
computacionalmente económica. Apesar da sua flexibilidade, esta \emph{framework} carecia (até à publicação
de contribuições recentes: \autocite{semisuplearning_nflows}, \autocite{RAD}) de uma forma
de introduzir estrutura discreta (como a que se pode encontrar em modelos de mistura)
nos modelos que permite construir, de forma não-supervisionada. Este trabalho
pretende ser um passo nessa direcção, utilizando \emph{fluxos normalizantes} como
componentes num modelo de mistura, e descrevendo um procedimento para o treino
desse modelo. Este procedimento é baseado em inferência variacional e usa um posterior
variacional parameterizado por uma rede neuronal. Como se tornará evidente, este
modelo adequa-se naturalmente às tarefas de estimação de densidades de probabilidade (multimodais),
aprendizagem semi-supervisionada e aglomeração de dados. O modelo proposto é
avaliado em dois conjuntos de dados sintéticos e um conjunto de dados real.
\vfill

\textbf{\Large Palavras-chave:} Modelos generativos profundos, fluxos normalizantes,
inferência variacional, modelação probabilística, aprendizagem automática.
