\section{Discussion}

The main shortcoming of the proposed model is that the variational posterior does
not always partition the space in the intuitively correct manner. Potentially,
this could be improved by using a consistency loss regularization term. In fact,
this idea has been pursued in \autocite{semisuplearning_nflows}.
During the experimentation phase, it was found that a balance between the complexity of the variational
posterior and that of the components of the mixture, is crucial for the
convergence to interesting solutions. This is intuitive: if the components are
too complex, the variational posterior tends to ignore most of them and assigns
most points to a single or few components. In principle, the model should be
able to ignore redundant components, if the complexities of the posterior and the flows
are well calibrated. This needs to be evaluated empirically. The effect of using
different architectures for the neural networks also requires further study and
evaluation.
