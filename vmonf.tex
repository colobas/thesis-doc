\section{Variational Mixture of Normalizing Flows}
\label{section:vmonf}
Consider a mixture of $K$ normalizing flows. Let $p(\bm{x} | c)$ be the likelihood
of data point $\bm{x}$ under the flow indexed by $c$, where $c \in \{1, 2, ..., K\}$.
Let $q(c|\bm{x})$ be a neural network with a $K$-class softmax output. Using $q(c|\bm{x})$
as a variational posterior over $c$, the corresponding evidence lower bound (ELBO) is given by:
\begin{align}
    \text{ELBO} &= \mathbb{E}_q [\log p(\bm{x}|c)] + \mathbb{E}_q [\log p(c)] - \mathbb{E}_q [\log q(c|\bm{x})]
        \label{eq:threepartelbo} \\
    &= \sum_{c=1}^K q(c|\bm{x})\big(\log p(\bm{x}|c) + \log p(c) - \log q(c|\bm{x})\big). \label{eq:simplerelbo}
\end{align}
All the terms in \eqref{eq:simplerelbo} are trivial: $q(c|\bm{x})$ is the
forward-pass of a neural network; $\log p(\bm{x}|c)$ is the log-likelihood of $\bm{x}$
under the normalizing flow indexed by $c$; $\log p(c)$ is the log-prior of the
component weights, set by the modeller\footnote{When there is no \emph{a priori}
knowledge about the true component weights, the best assumption is that they
correspond to a uniform distribution}; $- \log q(c|\bm{x})$ is the negative
logarithm of the output of the encoder.

We denote this model by \emph{variational mixture of normalizing flows} (VMoNF).
In a similar fashion to the variational auto-encoder, proposed by \textcite{vaepaper},
a VMoNF is fitted by jointly optimizing the parameters of the variational
posterior $q(c|\bm{x})$ and the parameters of the generative process
$p(\bm{x}|c)$, so as to maximize \eqref{eq:simplerelbo}. This is done by leveraging
modern automatic differentiation frameworks, e.g. \autocite{pytorch}.
After training, the variational posterior naturally induces a clustering on
the data, and can be directly used to assign new data points to the discovered
clusters. Moreover, each of the fitted components can be used to generate
samples from the cluster it \q{specialized} in.
